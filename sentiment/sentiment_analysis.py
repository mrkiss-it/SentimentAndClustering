# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ed5LAIlqKOkULAp3sgcnkQXg9FEZPnGf

**Yêu cầu 1:** Các công ty
đang nhận nhiều đánh giá
(review) từ ứng viên/nhân
viên đăng trên ITViec →
Dựa trên những thông tin
này để phân tích cảm xúc
(tích cực, tiêu cực, trung
tính).

**Thư viện sử dụng:**
* numpy, pandas, matplotlib, seaborn
* underthesea
* wordcloud
* scikit-learn (sklearn)
* ...

**Bước 1: Business Understanding**

**Mục tiêu:** Xây dựng mô hình dự đoán giúp itviec
và các công ty đối tác có thể biết được những phản hồi
nhanh chóng của nhân viên/ ứng viên về công ty mình (tích
cực, tiêu cực hay trung tính).

**Bước 2: Data Understanding/ Acquire**

* Dữ liệu được cung cấp sẵn trong các tập tin:
  * Overview_Companies.xls
  * Overview_Reviews.xls
  * Reviews.xls
* Kèm theo đó là file mô tả: Mô tả bộ dữ liệu ITViec.pdf

Sentiment analysis với các thuật toán thuộc nhóm
Supervised Learning – Classification như: Naïve
Bayes, KNN, Logistic Regression, Tree Algorithms,
SVM ...

**Bước 3: Data preparation/ Prepare**

* chuẩn hóa text (tiếng Việt, tiếng Anh)
* xử lý làm sạch dữ liệu

**Bước 4&5: Modeling & Evaluation/ Analyze & Report**

* Xây dựng các Classification model cho Sentiment
Analysis (chọn ít nhất 3 thuật toán)
* Thực hiện/ đánh giá kết quả các Classification
model: R-squared, ccc, precision, recall, f1, confusion matrix,
ROC curve...
* Nếu dữ liệu mất cân bằng gây ảnh hưởng đến kết quả thì
xem xét thêm việc xử lý mất cân bằng
* So sánh các kết quả
* Có thể đề xuất thêm các thuật toán mới

=> Kết luận

**Bước 6: Deployment & Feedback/ Act**

Đưa ra những cải tiến phù hợp để nâng cao sự
hài lòng của nhân viên/ ứng viên → Môi trường
làm việc tốt, thu hút ứng viên, giữ chân nhân
viên...

* Nhận xét tích cực và tiêu cực kèm wordcloud của từng
loại, các keyword chính liên quan, các trực quan và
thống kê cần thiết...

  → Các phân tích, đề xuất cho công ty

# For Google Colab
"""

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt

def check_wordcloud(data, col_name):
  text = " ".join(data)
  wc = WordCloud(width=800, height=400, background_color='white').generate(text)
  plt.imshow(wc, interpolation='bilinear')
  plt.axis("off")
  plt.title("WordCloud của " + col_name)
  plt.show()

"""## STEP 2: Clean Text (English + Vietnamese)"""

import re

def clean_text(text):
    # Loại bỏ các ký tự đặc biệt không cần thiết
    text = re.sub(r'[^\w\s\.,!?;:]', ' ', text)

    # Xóa URL
    text = re.sub(r'http\S+|www\.\S+', '', text)

    # Chuẩn hóa khoảng trắng
    text = re.sub(r'\s+', ' ', text)

    # Loại bỏ khoảng trắng đầu/cuối
    text = text.strip()

    return text

import regex
def normalize_repeated_characters(text):
    """
    Chuẩn hóa các từ có ký tự lặp liên tiếp
    Ví dụ: "lònggggg" -> "lòng", "thiệtttt" -> "thiệt"
    """
    # Xử lý ký tự Việt Nam (bao gồm dấu)
    # Thay thế 3+ ký tự liên tiếp bằng 1 ký tự
    text = re.sub(r'([aăâeêiouôơưyàáảãạằắẳẵặầấẩẫậèéẻẽẹềếểễệìíỉĩịòóỏõọồốổỗộờớởỡợùúủũụừứửữựỳýỷỹỵ])\1{2,}', r'\1', text, flags=re.IGNORECASE)

    # Xử lý consonant
    text = re.sub(r'([bcdfghjklmnpqrstvwxz])\1{2,}', r'\1', text, flags=re.IGNORECASE)

    return text

import re

def normalize_punctuation(text):
    """Chuẩn hóa các dấu câu"""

    # Chuẩn hóa dấu chấm
    text = re.sub(r'\.{2,}', '.', text)

    # Chuẩn hóa dấu hỏi chấm
    text = re.sub(r'\?{2,}', '?', text)

    # Chuẩn hóa dấu cảm thán
    text = re.sub(r'!{2,}', '!', text)

    # Chuẩn hóa dấu phẩy
    text = re.sub(r',{2,}', ',', text)

    # Loại bỏ dấu nháy đơn (nếu cần)
    text = text.replace("'", "")

    # Chuẩn hóa khoảng trắng xung quanh dấu câu
    text = re.sub(r'\s*([.!?,:;])\s*', r'\1 ', text)
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

import unicodedata

def normalize_vietnamese(text):
    """Chuẩn hóa unicode tiếng Việt sử dụng unicodedata"""
    return unicodedata.normalize('NFC', text)

import html

def process_special_chars(text):
    # Decode HTML entities
    text = html.unescape(text)

    # Xử lý emoji (tùy task)
    # Có thể giữ nguyên hoặc thay thế bằng text mô tả

    return text

def process_basic_text(text, max_length=256):
    # 1. Làm sạch cơ bản
    text = clean_text(text)

    # 2. Chuẩn hóa ký tự lặp
    text = normalize_repeated_characters(text)

    # 3. Chuẩn hóa dấu câu
    text = normalize_punctuation(text)

    # 4. Chuẩn hóa tiếng Việt
    text = normalize_vietnamese(text)

    # 5. Xử lý ký tự đặc biệt
    text = process_special_chars(text)

    return text


from pathlib import Path

# Đọc từ khóa tích cực
pos_words = set(Path("files/positive_words.txt").read_text("utf-8").splitlines())

# Đọc từ khóa tiêu cực
neg_words = set(Path("files/negative_words.txt").read_text("utf-8").splitlines())

print(f"> Loaded {len(pos_words)} positive, {len(neg_words)} negative keywords.")

# Đọc emoji tích cực
pos_emoji = set(Path("files/positive_emoji.txt").read_text("utf-8").splitlines())

# Đọc emoji tiêu cực
neg_emoji = set(Path("files/negative_emoji.txt").read_text("utf-8").splitlines())

print(f"Loaded {len(pos_emoji)} positive emojis, {len(neg_emoji)} negative emojis")

def calc_sentiment_features(text):
    '''
      pos_w, neg_w: số từ tích cực/tiêu cực

      pos_e, neg_e: số emoji tích cực/tiêu cực

      total_we: tổng số từ và emoji mang cảm xúc

      ratio_all: tỷ lệ cân bằng tích cực – tiêu cực
    '''

    toks = text.lower().strip()
    pos_w = sum(t in toks for t in pos_words)
    neg_w = sum(t in toks for t in neg_words)
    pos_e = sum(text.count(e) for e in pos_emoji)
    neg_e = sum(text.count(e) for e in neg_emoji)

    total_w = pos_w + neg_w
    total_e = pos_e + neg_e
    total_we = total_w + total_e

    ratio_words = (pos_w - neg_w) / total_w if total_w else 0
    ratio_emoji = (pos_e - neg_e) / total_e if total_e else 0
    ratio_all = (pos_w + pos_e - neg_w - neg_e) / total_we if total_we else 0

    return pos_w, neg_w, pos_e, neg_e, total_we, ratio_all


from langdetect import detect
def detect_lang_safe(text):
    # Hàm detect language đơn giản, bạn có thể dùng langdetect hoặc rule riêng
    try:
        return detect(text)
    except:
        return ''


import re
from collections import deque

def recursive_split_sentences(text, lang='vi'):
    """
    Recursively split sentences based on patterns for the specified language.

    Args:
        text (str): Input text to split
        lang (str): Language code ('vi' for Vietnamese, 'en' for English)

    Returns:
        list: List of split sentences
    """
    # Define patterns for each language
    if lang == 'vi':
        patterns = [
            r'\bmỗi tội\b.*?\bdo\b.*?(?=,|\.|$)',
            r'\bdo\b.*?\bnên\b.*?(?=,|\.|$)',
            r'\bvì\b.*?\bnên\b.*?(?=,|\.|$)',
            r'\bmặc dù\b.*?\bnhưng\b.*?(?=,|\.|$)',
            r'\bnếu\b.*?\bthì\b.*?(?=,|\.|$)',
        ]
    elif lang == 'en':
        patterns = [
            r'\bif\b.*?\bthen\b.*?(?=,|\.|$)',
            r'\bwhen\b.*?\bthen\b.*?(?=,|\.|$)',
            r'\bsince\b.*?\btherefore\b.*?(?=,|\.|$)',
            r'\bbecause\b.*?\bso\b.*?(?=,|\.|$)',
            r'\bas\b.*?\bso\b.*?(?=,|\.|$)',
            r'\balthough\b.*?\byet\b.*?(?=,|\.|$)',
            r'\bthough\b.*?\bstill\b.*?(?=,|\.|$)',
            r'\bwhile\b.*?\bhowever\b.*?(?=,|\.|$)',
            r'\bunless\b.*?\botherwise\b.*?(?=,|\.|$)',
            r'\beven if\b.*?\bstill\b.*?(?=,|\.|$)',
        ]
    else:
        # Default to Vietnamese patterns
        patterns = [
            r'\bmỗi tội\b.*?\bdo\b.*?(?=,|\.|$)',
            r'\bdo\b.*?\bnên\b.*?(?=,|\.|$)',
            r'\bvì\b.*?\bnên\b.*?(?=,|\.|$)',
            r'\bmặc dù\b.*?\bnhưng\b.*?(?=,|\.|$)',
            r'\bnếu\b.*?\bthì\b.*?(?=,|\.|$)',
        ]

    queue = deque([text.strip()])
    results = []

    while queue:
        sentence = queue.popleft().strip(" ,.")

        matched = False
        for pattern in patterns:
            match = re.search(pattern, sentence, re.IGNORECASE)
            if match:
                matched = True
                # Phần trước mẫu
                before = sentence[:match.start()].strip(" ,.")
                # Phần trùng với mẫu
                middle = match.group().strip(" ,.")
                # Phần sau mẫu
                after = sentence[match.end():].strip(" ,.")

                if before:
                    queue.append(before)
                results.append(middle + ".")  # Đưa thẳng vào kết quả, không xử lý lại
                if after:
                    queue.append(after)
                break  # Chỉ dùng 1 pattern mỗi vòng

        if not matched:
            if sentence:
                results.append(sentence + ".")

    return results


def split_sentences_by_meaning(text, lang='vi'):
    """
    Tách câu theo dấu câu và một số liên từ chia ý.

    Args:
        text (str): Input text to split
        lang (str): Language code ('vi' for Vietnamese, 'en' for English)

    Returns:
        list: List of split sentences
    """
    # Danh sách các liên từ và từ nối phổ biến dùng để tách ý
    if lang == 'vi':
        split_keywords = [
            r'\bnhưng\b', r'\btuy nhiên\b', r'\btuy\b', r'\bmặc dù\b',
            r'\bdù\b', r'\bvì vậy\b', r'\bvì thế\b', r'\bdo đó\b',
            r'\bcũng\b', r'\bsong\b',
        ]
    elif lang == 'en':
        split_keywords = [
            r'\bbut\b', r'\bhowever\b', r'\bnevertheless\b', r'\bnonetheless\b',
            r'\balthough\b', r'\bthough\b', r'\beven though\b', r'\bwhile\b',
            r'\bwhereas\b', r'\btherefore\b', r'\bthus\b', r'\bhence\b',
            r'\bconsequently\b', r'\bmoreover\b', r'\bfurthermore\b', r'\bin addition\b',
            r'\byet\b', r'\bso\b', r'\bfor\b', r'\bnor\b',
            r'\botherwise\b', r'\bmeanwhile\b', r'\bon the other hand\b',
            r'\bin contrast\b', r'\bsimilarly\b', r'\blikewise\b'
        ]
    else:
        # Default to Vietnamese keywords
        split_keywords = [
            r'\bnhưng\b', r'\btuy nhiên\b', r'\btuy\b', r'\bmặc dù\b',
            r'\bdù\b', r'\bvì vậy\b', r'\bvì thế\b', r'\bdo đó\b',
            r'\bcũng\b', r'\bsong\b',
        ]

    # Bước 1: Chuẩn hóa văn bản
    text = text.strip()
    if not text:
        return []

    # Bước 2: Tách câu theo dấu ngắt (., !, ?)
    sentences = re.split(r'(?<=[.!?])\s+', text)

    final_sentences = []

    # Bước 3: Với mỗi câu, tiếp tục tách nếu có liên từ mang nhiều ý
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        # Ghép các keyword lại thành regex OR
        pattern = '|'.join(split_keywords)

        # Tách nếu có từ nối nhiều ý
        sub_sentences = re.split(pattern, sentence, flags=re.IGNORECASE)

        # Flatten the list of lists and strip each element
        for sub_sentence in sub_sentences:
            split_do_neu_result = recursive_split_sentences(sub_sentence, lang)
            final_sentences.extend([s.strip() for s in split_do_neu_result if s.strip()])

    return final_sentences


# Example usage
if __name__ == "__main__":
    # Test Vietnamese
    vietnamese_text = """
    Vì là công ty IT service nên có nhiều thứ để trau dồi và cải thiện chuyên môn Rất ít khi OT, nếu có OT thì trả lương OT đầy đủ theo luật
    """

    print("=== VIETNAMESE TEST ===")
    result_vi = split_sentences_by_meaning(vietnamese_text, lang='vi')
    for i, sentence in enumerate(result_vi, 1):
        print(f"{i}. {sentence}")

    # Test English
    english_text = """
    If you study hard then you will pass the exam, but if you don't study then you might fail.
    Although it was raining, we went to the park, however we got wet.
    """

    print("\n=== ENGLISH TEST ===")
    result_en = split_sentences_by_meaning(english_text, lang='en')
    for i, sentence in enumerate(result_en, 1):
        print(f"{i}. {sentence}")



import nltk
from nltk.corpus import stopwords

# Tải bộ stopwords tiếng Anh (chỉ chạy 1 lần)
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')

# Hàm tiện ích để load tất cả dữ liệu
def load_processing_data():
    """Load tất cả dữ liệu cần thiết cho việc xử lý văn bản"""

    # LOAD EMOJICON
    try:
        with open('files/emojicon.txt', 'r', encoding="utf8") as file:
            emoji_lst = file.read().split('\n')
        emoji_dict = {}
        for line in emoji_lst:
            if '\t' in line and line.strip():  # Kiểm tra format và không rỗng
                line = normalize_vietnamese(line)  # Chuẩn hóa unicode
                parts = line.split('\t', 1)  # Chỉ split lần đầu
                if len(parts) == 2:
                    key, value = parts
                    emoji_dict[key] = str(value)
    except FileNotFoundError:
        print("Warning: emojicon.txt not found, using empty emoji dict")
        emoji_dict = {}

    # LOAD TEENCODE
    try:
        with open('files/teencode.txt', 'r', encoding="utf8") as file:
            teen_lst = file.read().split('\n')
        teen_dict = {}
        for line in teen_lst:
            if '\t' in line and line.strip():
                line = normalize_vietnamese(line)  # Chuẩn hóa unicode
                parts = line.split('\t', 1)
                if len(parts) == 2:
                    key, value = parts
                    teen_dict[key] = str(value)
    except FileNotFoundError:
        print("Warning: teencode.txt not found, using empty teen dict")
        teen_dict = {}

    #LOAD TRANSLATE ENGLISH -> VNMESE
    try:
        with open('files/english-vnmese.txt', 'r', encoding="utf8") as file:
            english_lst = file.read().split('\n')
        english_dict = {}
        for line in english_lst:
            if '\t' in line and line.strip():
                line = normalize_vietnamese(line)  # Chuẩn hóa unicode
                parts = line.split('\t', 1)
                if len(parts) == 2:
                    key, value = parts
                    english_dict[key] = str(value)
    except FileNotFoundError:
        print("Warning: teencode.txt not found, using empty teen dict")
        english_dict = {}

    # LOAD WRONG WORDS
    try:
        with open('files/wrong-word.txt', 'r', encoding="utf8") as file:
            wrong_lst = file.read().split('\n')
        wrong_lst = [normalize_vietnamese(word.strip()) for word in wrong_lst if word.strip()]  # Chuẩn hóa unicode
    except FileNotFoundError:
        print("Warning: wrong-word.txt not found, using empty wrong list")
        wrong_lst = []


    # LOAD STOPWORDS
    try:
        with open('files/vietnamese-stopwords.txt', 'r', encoding="utf8") as file:
            stopwords_vi = file.read().split('\n')
        stopwords_vi = [normalize_vietnamese(word.strip()) for word in stopwords_vi if word.strip()]  # Chuẩn hóa unicode
        # Loại bỏ các từ phủ định trong stopword
        negations_vi = {'không', 'chẳng', 'chả', 'đâu', 'chưa', 'đừng', 'khỏi'}
        stopwords_vi = [word for word in stopwords_vi if word not in negations_vi]
        # Loải bỏ các từ quan trọng
        keep_words = {'rất', 'không', 'nên', 'có', 'tốt', 'xấu', 'làm việc', 'áp lực', 'thoải mái', 'ổn', 'hài lòng', 'khó', 'tệ', 'cực kỳ', 'nhiều', 'ít', 'cao', 'thấp', 'khá'}
        stopwords_vi = [word for word in stopwords_vi if word not in keep_words]
    except FileNotFoundError:
        print("Warning: vietnamese-stopwords.txt not found, using empty stopwords list")
        stopwords_vi = []

    # Lấy bộ stopwords tiếng Anh
    stopwords_en = set(stopwords.words('english'))

    # Loại bỏ từ phủ định khỏi bộ stopwords
    negations = {'not', 'no', 'nor', 'don', 'didn', 'doesn', "don't", "didn't", "doesn't"}
    stopwords_en = [word for word in stopwords_en if word not in negations]
    # Loại bỏ các từ quan trọng
    keep_words = {'not', 'very', 'no', 'never', 'good', 'bad', 'great', 'poor', 'excellent', 'happy', 'sad', 'stressful', 'comfortable', 'satisfied', 'unhappy', 'awful', 'amazing'}
    stopwords_en = [word for word in stopwords_en if word not in keep_words]

    return emoji_dict, teen_dict, english_dict, wrong_lst, stopwords_vi, stopwords_en

emoji_dict, teen_dict, english_dict, wrong_lst, stopwords_vi, stopwords_en = load_processing_data()

import re
from underthesea import pos_tag as vi_pos_tag
from underthesea import word_tokenize as vi_tokenize
from nltk import pos_tag as en_pos_tag
from nltk.tokenize import word_tokenize as en_tokenize

# POS giữ lại
VI_POS_KEEP = {'N', 'V', 'A', 'R'}
EN_POS_KEEP = {'NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS'}

# Từ ngắn quan trọng (negations, intensifiers)
VI_KEEP_SHORT = {'không', 'rất', 'quá', 'hơi', 'ít', 'cao', 'thấp', 'khá'}
EN_KEEP_SHORT = {'not', 'no', 'very', 'too'}

def normalize_text(text, dict_map):
    # Thay thế từ theo dict_map (teencode, sai chính tả, emoji...)
    pattern = re.compile(r'\b(' + '|'.join(re.escape(key) for key in dict_map.keys()) + r')\b', flags=re.IGNORECASE)
    def replace_func(match):
        key = match.group(0).lower()
        return dict_map.get(key, key)
    return pattern.sub(replace_func, text)

def replace_emoji(text, emoji_dict):
    # Thay emoji từng ký tự
    for emo, rep in emoji_dict.items():
        text = text.replace(emo, ' ' + rep + ' ')
    return text

def remove_wrong_words(text, wrong_lst):
    if not text:
        return ""
    words = text.split()
    cleaned_words = [word for word in words if word not in wrong_lst]
    return " ".join(cleaned_words)

def process_text(text):
    if not text or not text.strip():
        return ""

    lang = detect_lang_safe(text)

    # Bước 1: Thay emoji thành từ tương ứng
    text = replace_emoji(text, emoji_dict)

    # Bước 2: Chuẩn hóa teencode
    text = normalize_text(text, teen_dict)

    # Bước 3: Chuẩn hóa từ sai chính tả
    if lang == 'vi':
        text = remove_wrong_words(text, wrong_lst)
    else:
        text = normalize_text(text, english_dict)

    if lang == 'vi':
        tokens = vi_tokenize(text, format="text")
        tagged = vi_pos_tag(tokens)
        # Lọc từ theo POS và từ ngắn quan trọng
        results = [word for word, pos in tagged if (pos in VI_POS_KEEP or word.lower() in VI_KEEP_SHORT)]
        # Loại bỏ stopword
        results = [w for w in results if w.lower() not in stopwords_vi]
    else:
        tokens = en_tokenize(text)
        tagged = en_pos_tag(tokens)
        results = [word for word, pos in tagged if (pos in EN_POS_KEEP or word.lower() in EN_KEEP_SHORT)]
        results = [w for w in results if w.lower() not in stopwords_en]

    return ' '.join(results)

# Ví dụ chạy thử
if __name__ == '__main__':
    vi_example = "Môi trường làm việc rất tốt 😊, nhưng áp lực hok nhỏ và lương thấp."
    en_example = "The working environment is very good 😊, but the pressure is high and the salary is low."

    print("Vietnamese:", process_text(vi_example))
    print("English:", process_text(en_example))

import re
from underthesea import pos_tag as vi_pos_tag
from underthesea import word_tokenize as vi_tokenize
from nltk import pos_tag as en_pos_tag
from nltk.tokenize import word_tokenize as en_tokenize

def process_tagged_sentence(tagged_words, language='vi'):
    """
    Xử lý câu đã được gán nhãn từ loại để tạo câu có nghĩa với phân cụm ý nghĩa
    Hỗ trợ cả tiếng Việt và tiếng Anh

    Args:
        tagged_words: Danh sách các tuple (từ, nhãn_từ_loại)
        language: 'vi' cho tiếng Việt, 'en' cho tiếng Anh

    Returns:
        str: Câu đã được xử lý, phân thành các cụm ý nghĩa
    """

    # Định nghĩa các từ loại cần giữ lại (có nghĩa) cho cả tiếng Việt và tiếng Anh
    meaningful_tags = {
        'N', 'NN', 'NNS', 'NNP', 'NNPS',  # Danh từ (Noun)
        'V', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Động từ (Verb)
        'A', 'JJ', 'JJR', 'JJS',  # Tính từ (Adjective)
    }

    # Định nghĩa các từ cụ thể cần loại bỏ theo ngôn ngữ
    if language == 'vi':
        stop_words = {
            'có_thể', 'ở', 'từ', 'vì', 'mỗi', 'của', 'với', 'theo', 'trong', 'trên', 'dưới'
        }
        # Trạng từ quan trọng cần giữ
        important_adverbs = {'không', 'khá', 'rất', 'tương_đối', 'khá_là', 'đã', 'sẽ', 'đang'}
        # Từ quan trọng cần giữ bất kể từ loại
        important_words = {'nên', 'và', 'hoặc', 'cũng', 'thì', 'là', 'được'}
        # Từ loại dấu câu và giới từ cần bỏ qua
        skip_tags = {'CH', 'E', 'C'}
    else:  # English
        stop_words = {
            'of', 'in', 'on', 'at', 'by', 'for', 'with', 'from', 'to', 'about',
            'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between'
        }
        # Trạng từ quan trọng cần giữ
        important_adverbs = {'not', 'very', 'quite', 'really', 'too', 'so', 'already', 'still', 'just'}
        # Từ quan trọng cần giữ bất kể từ loại
        important_words = {'and', 'or', 'but', 'should', 'must', 'can', 'will', 'would', 'could', 'is', 'are', 'was', 'were', 'be', 'been', 'being'}
        # Từ loại dấu câu và giới từ cần bỏ qua
        skip_tags = {'.', ',', ':', ';', '!', '?', 'IN', 'TO', 'CC'}

    meaningful_words = []
    current_chunk = []
    check_tags = set()

    for i, (word, tag) in enumerate(tagged_words):
        # Bỏ qua các từ trong danh sách stop words
        if word.lower() in stop_words:
            continue

        # Xử lý dấu phẩy - kết thúc cụm hiện tại
        if word == ',' and current_chunk:
            if len(check_tags) == 1 and meaningful_words:
                last_text = meaningful_words.pop()
                meaningful_words.append(last_text + ', ' + ' '.join(current_chunk))
            else:
                meaningful_words.append(' '.join(current_chunk))
            current_chunk = []
            check_tags = set()
            continue

        # Bỏ qua dấu chấm cuối câu
        if word == '.' and i == len(tagged_words) - 1:
            continue

        # Giữ lại từ quan trọng bất kể từ loại
        if word.lower() in important_words:
            current_chunk.append(word)
            check_tags.add(tag)
            continue

        # Giữ lại các từ có nghĩa
        if tag in meaningful_tags:
            # Kiểm tra trạng từ quan trọng
            if tag in {'R', 'RB', 'RBR', 'RBS'} and word.lower() not in important_adverbs:
                continue
            current_chunk.append(word)
            check_tags.add(tag)
        elif tag not in skip_tags:  # Giữ lại các tag khác nếu không phải dấu câu, giới từ, liên từ
            current_chunk.append(word)
            check_tags.add(tag)

    # Thêm cụm cuối cùng nếu còn
    if current_chunk:
        if len(check_tags) == 1 and meaningful_words:
              last_text = meaningful_words.pop()
              meaningful_words.append(last_text + ', ' + ' '.join(current_chunk))
        else:
            meaningful_words.append(' '.join(current_chunk))

    # Hàm viết hoa chữ cái đầu
    def capitalize_first_word(text):
        if not text:
            return text
        words = text.split()
        if words and len(words[0]) > 0:
            first_word = words[0]
            words[0] = first_word[0].upper() + first_word[1:]
        return ' '.join(words)

    # Tạo câu với định dạng mong muốn và viết hoa
    if len(meaningful_words) == 1:
        return capitalize_first_word(meaningful_words[0]) + "."
    elif len(meaningful_words) == 2:
        return (capitalize_first_word(meaningful_words[0]) + ". " +
                capitalize_first_word(meaningful_words[1]) + ".")
    elif len(meaningful_words) >= 3:
        # Nối 2 cụm đầu bằng dấu phẩy, các cụm sau bằng dấu chấm
        result = capitalize_first_word(meaningful_words[0]) + ", " + capitalize_first_word(meaningful_words[1]) + ". "
        for chunk in meaningful_words[2:]:
            result += capitalize_first_word(chunk) + ". "
        return result.rstrip()

    return ""


def batch_process_sentences(sentences_data, language='vi'):
    """
    Xử lý nhiều câu cùng lúc cho cả tiếng Việt và tiếng Anh

    Args:
        sentences_data: Danh sách các câu đã được tag
        language: 'vi' cho tiếng Việt, 'en' cho tiếng Anh
    """
    results = []

    for tagged_words in sentences_data:
        processed = process_tagged_sentence(tagged_words, language)
        results.append(processed)

    return results


def advanced_process_with_context(tagged_words, keep_context_words=False, language='vi'):
    """
    Xử lý nâng cao với tùy chọn giữ lại một số từ ngữ cảnh
    Hỗ trợ cả tiếng Việt và tiếng Anh

    Args:
        tagged_words: Danh sách các tuple (từ, nhãn_từ_loại)
        keep_context_words: Có giữ lại một số từ ngữ cảnh không
        language: 'vi' cho tiếng Việt, 'en' cho tiếng Anh

    Returns:
        str: Câu đã được xử lý
    """

    # Từ loại chính cần giữ cho cả tiếng Việt và tiếng Anh
    core_tags = {
        'N', 'NN', 'NNS', 'NNP', 'NNPS',  # Danh từ
        'V', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Động từ
        'A', 'JJ', 'JJR', 'JJS'  # Tính từ
    }

    # Từ loại phụ có thể giữ tùy theo ngữ cảnh
    context_tags = {'R', 'L', 'RB', 'RBR', 'RBS', 'DT', 'PDT', 'WDT'} if keep_context_words else set()

    # Từ quan trọng cần giữ bất kể từ loại theo ngôn ngữ
    if language == 'vi':
        important_words = {
            'không', 'khá', 'tốt', 'cao', 'thấp', 'nhiều', 'ít', 'nên', 'rất', 'đã', 'sẽ', 'được'
        }
    else:  # English
        important_words = {
            'not', 'quite', 'good', 'high', 'low', 'many', 'few', 'should', 'very', 'already', 'will', 'would'
        }

    meaningful_words = []

    for word, tag in tagged_words:
        # Luôn giữ từ quan trọng
        if word.lower() in important_words:
            meaningful_words.append(word)
            continue

        # Giữ từ loại chính
        if tag in core_tags:
            meaningful_words.append(word)
            continue

        # Giữ từ loại ngữ cảnh nếu được yêu cầu
        if tag in context_tags:
            meaningful_words.append(word)
            continue

    return ' '.join(meaningful_words)


def detect_language(tagged_words):
    """
    Tự động phát hiện ngôn ngữ dựa trên POS tags

    Args:
        tagged_words: Danh sách các tuple (từ, nhãn_từ_loại)

    Returns:
        str: 'vi' cho tiếng Việt, 'en' cho tiếng Anh
    """
    english_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS'}
    vietnamese_tags = {'N', 'V', 'A', 'R', 'L', 'CH', 'E', 'C'}

    tags_in_sentence = {tag for _, tag in tagged_words}

    english_count = len(tags_in_sentence.intersection(english_tags))
    vietnamese_count = len(tags_in_sentence.intersection(vietnamese_tags))

    return 'en' if english_count > vietnamese_count else 'vi'


def auto_process_tagged_sentence(tagged_words, keep_context_words=False):
    """
    Tự động phát hiện ngôn ngữ và xử lý câu

    Args:
        tagged_words: Danh sách các tuple (từ, nhãn_từ_loại)
        keep_context_words: Có giữ lại một số từ ngữ cảnh không

    Returns:
        str: Câu đã được xử lý
    """
    language = detect_language(tagged_words)
    return process_tagged_sentence(tagged_words, language)


# Ví dụ sử dụng
if __name__ == "__main__":
    # Test với dữ liệu mẫu tiếng Việt
    sentence1_vi = [('Cần', 'V'), ('đánh_giá', 'V'), ('KPI khách_quan', 'N'), ('và', 'C'), ('phân_chia', 'V'), ('công_việc', 'V'), ('chuyên_nghiệp', 'N')
                 , (',', 'CH'), ('match', 'V'), ('với', 'E'), ('khả_năng', 'N'), (',', 'CH'), ('trình_độ', 'N'), ('của', 'E'), ('nhân_viên', 'N'), ('.', 'CH')]

    sentence2_vi = [('Môi_trường', 'N'), ('thoải_mái', 'N'), (',', 'CH'), ('ít', 'A'), ('áp_lực', 'N'), (',', 'CH'), ('có_thể', 'V'), ('làm_việc hybrid', 'N'), (',', 'CH')
                , ('lương', 'N'), ('nên', 'C'), ('deal', 'V'), ('tốt', 'A'), ('từ', 'E'), ('lúc', 'N'), ('đầu', 'N'), (',', 'CH'), ('vì', 'E'), ('mức', 'N'), ('tăng', 'V')
                , ('mỗi', 'L'), ('năm', 'N'), ('không', 'R'), ('cao', 'A'), ('.', 'CH')]

    # Test với dữ liệu mẫu tiếng Anh
    sentence1_en = [('Need', 'VB'), ('to', 'TO'), ('evaluate', 'VB'), ('objective', 'JJ'), ('KPI', 'NN'), ('and', 'CC'), ('divide', 'VB'), ('work', 'NN'), ('professionally', 'RB'),
                    (',', ','), ('match', 'VB'), ('with', 'IN'), ('ability', 'NN'), (',', ','), ('level', 'NN'), ('of', 'IN'), ('employee', 'NN'), ('.', '.')]

    sentence2_en = [('Comfortable', 'JJ'), ('environment', 'NN'), (',', ','), ('less', 'JJR'), ('pressure', 'NN'), (',', ','), ('can', 'MD'), ('work', 'VB'), ('hybrid', 'JJ'), (',', ','),
                    ('salary', 'NN'), ('should', 'MD'), ('deal', 'VB'), ('good', 'JJ'), ('from', 'IN'), ('beginning', 'NN'), (',', ','), ('because', 'IN'), ('increase', 'NN'),
                    ('each', 'DT'), ('year', 'NN'), ('not', 'RB'), ('high', 'JJ'), ('.', '.')]

    # Test xử lý câu đơn lẻ tiếng Việt
    print("=== Test xử lý câu đơn lẻ tiếng Việt ===")
    result1_vi = process_tagged_sentence(sentence1_vi, 'vi')
    print(f"Câu 1 VI: {result1_vi}")

    result2_vi = process_tagged_sentence(sentence2_vi, 'vi')
    print(f"Câu 2 VI: {result2_vi}")

    # Test xử lý câu đơn lẻ tiếng Anh
    print("\n=== Test xử lý câu đơn lẻ tiếng Anh ===")
    result1_en = process_tagged_sentence(sentence1_en, 'en')
    print(f"Câu 1 EN: {result1_en}")

    result2_en = process_tagged_sentence(sentence2_en, 'en')
    print(f"Câu 2 EN: {result2_en}")

    # Test tự động phát hiện ngôn ngữ
    print("\n=== Test tự động phát hiện ngôn ngữ ===")
    auto_result1_vi = auto_process_tagged_sentence(sentence1_vi)
    print(f"Tự động VI: {auto_result1_vi}")

    auto_result1_en = auto_process_tagged_sentence(sentence1_en)
    print(f"Tự động EN: {auto_result1_en}")

    # Test xử lý batch
    print("\n=== Test xử lý batch ===")
    sentences_data_vi = [sentence1_vi, sentence2_vi]
    sentences_data_en = [sentence1_en, sentence2_en]

    batch_results_vi = batch_process_sentences(sentences_data_vi, 'vi')
    print("Batch VI:")
    for result in batch_results_vi:
        print(f"  {result}")

    batch_results_en = batch_process_sentences(sentences_data_en, 'en')
    print("Batch EN:")
    for result in batch_results_en:
        print(f"  {result}")

    # Test xử lý nâng cao
    print("\n=== Test xử lý nâng cao ===")
    advanced_result1_vi = advanced_process_with_context(sentence1_vi, keep_context_words=True, language='vi')
    print(f"Câu 1 VI (giữ ngữ cảnh): {advanced_result1_vi}")

    advanced_result2_en = advanced_process_with_context(sentence2_en, keep_context_words=False, language='en')

def normalize_text(text, dict_map):
    # Thay thế từ theo dict_map (teencode, sai chính tả, emoji...)
    pattern = re.compile(r'\b(' + '|'.join(re.escape(key) for key in dict_map.keys()) + r')\b', flags=re.IGNORECASE)
    def replace_func(match):
        key = match.group(0).lower()
        return dict_map.get(key, key)
    return pattern.sub(replace_func, text)

def replace_emoji(text, emoji_dict):
    # Thay emoji từng ký tự
    for emo, rep in emoji_dict.items():
        text = text.replace(emo, ' ' + rep + ' ')
    return text

def process_text(text, lang='vi'):
    if not text or not text.strip():
        return ""

    # Bước 1: Thay emoji thành từ tương ứng
    text = replace_emoji(text, emoji_dict)

    # Bước 2: Chuẩn hóa teencode
    text = normalize_text(text, teen_dict)

    if lang == 'vi':
        tokens = vi_tokenize(text, format="text")
        tagged = vi_pos_tag(tokens)
        results = process_tagged_sentence(tagged).split('.')
        results = [s.strip() for s in results if s.strip()]
    else:
        tokens = en_tokenize(text)
        tagged = en_pos_tag(tokens)
        results = process_tagged_sentence(tagged).split('.')
        results = [s.strip() for s in results if s.strip()]

    return results

def process_split_text(split_text, lang='vi'):
    """
    Xử lý nhiều câu cùng lúc

    """
    results = []
    for text in split_text:
        if not text or not text.strip():
            continue

        results.extend(process_text(text, lang))
    # loại bỏ câu trùng
    results = list(set(results))
    # nối thành 1 chuỗi
    return '. '.join(results)

# Oversampling nâng cao
from collections import Counter

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Define models
models = {
    "Naive Bayes": MultinomialNB(),
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "SVM": SVC(kernel='linear', probability=True, class_weight='balanced'),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
}

import seaborn as sns
def analyze_reviews(df, company_name=None, col_pred='Pred_FN'):
    """
    Phân tích sentiment từ nhận xét ứng viên/nhân viên.
    """
    if company_name:
        print(f"\n📌 Công ty: {company_name}")
        df_filtered = df[df['Company Name'] == company_name].copy()
        if df_filtered.empty:
            print("⚠️ Không tìm thấy dữ liệu.")
            return
    else:
        print("\n📊 Phân tích toàn bộ dữ liệu")
        df_filtered = df.copy()
        if df_filtered.empty:
            print("⚠️ Không có dữ liệu.")
            return

    # 1. Thống kê tổng quan sentiment
    print("\n📈 Thống kê cảm xúc:")
    sentiment_counts = df_filtered[col_pred].value_counts(normalize=True).reindex(['positive', 'neutral', 'negative'], fill_value=0) * 100
    sentiment_df = sentiment_counts.to_frame().T.rename(index={0: "Tỷ lệ (%)"})
    print(sentiment_df.style.format("{:.2f}%"))

    plt.figure(figsize=(8, 5))
    sns.countplot(data=df_filtered, x=col_pred, order=['positive', 'neutral', 'negative'], palette='viridis')
    plt.title("Phân bố cảm xúc nhận xét")
    plt.xlabel("Cảm xúc")
    plt.ylabel("Số lượng")
    plt.tight_layout()
    plt.show()

    # 2. Xử lý các nhận xét tích cực/tiêu cực
    def process_sentiment_group(label, color, word_list):
        reviews = df_filtered[df_filtered[col_pred] == label]['clean_advance_text2'].dropna()
        print(f"\n{'✨' if label == 'positive' else '👎'} Nhận xét {label}: {len(reviews)} nhận xét")
        if reviews.empty:
            print("  - Không có nhận xét nào.")
            return [], []

        text_combined = " ".join(reviews)
        try:
            stop_word = stopwords_vi + stopwords_en
            wordcloud = WordCloud(
                width=400, height=200, background_color='white',
                stopwords=stop_word, min_font_size=10
            ).generate(text_combined)
            print("  - Wordcloud:")
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis("off")
            plt.tight_layout()
            plt.show()
        except Exception as e:
            print(f"  - ❌ Lỗi tạo Wordcloud: {e}")

        all_keyword_counts = Counter()

        for word in word_list:
            if word.strip():
                word_lower = word.lower()
                count = text_combined.replace('_', ' ').lower().count(word_lower)
                if count > 0:
                  all_keyword_counts[word] = count

        common_keywords = all_keyword_counts.most_common(10)

        print("  - Top keywords:")
        print(pd.DataFrame(common_keywords, columns=['Keyword', 'Tần suất']))
        return reviews, common_keywords

    pos_reviews, top_pos_keywords = process_sentiment_group("positive", "green", pos_words)
    neg_reviews, top_neg_keywords = process_sentiment_group("negative", "red", neg_words)

    # 3. Phân tích theo công ty (chỉ khi phân tích toàn bộ)
    if company_name is None:
        print("\n🏢 Thống kê theo Công ty:")
        top_company_counts = df_filtered['Company Name'].value_counts().head(10)
        print(top_company_counts.to_frame(name='Số nhận xét'))

        avg_sentiment = df_filtered.groupby('Company Name')['sentiment_ratio'].mean().sort_values(ascending=False)
        print("\nTop 10 Công ty có Sentiment Ratio cao nhất:")
        print(avg_sentiment.head(10).to_frame(name='Trung bình Sentiment Ratio'))
        print("Top 10 Công ty có Sentiment Ratio thấp nhất:")
        print(avg_sentiment.tail(10).to_frame(name='Trung bình Sentiment Ratio'))

        top_bottom = avg_sentiment.head(5).index.tolist() + avg_sentiment.tail(5).index.tolist()
        df_top_bottom = df_filtered[df_filtered['Company Name'].isin(top_bottom)]

        plt.figure(figsize=(12, 6))
        sns.violinplot(data=df_top_bottom, x='sentiment_ratio', y='Company Name', palette='viridis')
        plt.title('Phân bố Sentiment Ratio theo Công ty')
        plt.xlabel('Sentiment Ratio')
        plt.ylabel('Công ty')
        plt.tight_layout()
        plt.show()

    # 4. Gợi ý & đề xuất
    print("\n💡 Đề xuất:")
    if company_name:
        print(f"- Tổng nhận xét: {len(df_filtered)}")
        print(f"- Tích cực: {sentiment_counts.get('positive', 0):.2f}%")
        print(f"- Tiêu cực: {sentiment_counts.get('negative', 0):.2f}%")

        if top_pos_keywords:
            print("  - Điểm mạnh:", ", ".join([kw for kw, _ in top_pos_keywords[:5]]))
        if top_neg_keywords:
            print("  - Cần cải thiện:", ", ".join([kw for kw, _ in top_neg_keywords[:5]]))

        if sentiment_counts.get('negative', 0) > sentiment_counts.get('positive', 0):
            print("  - 👉 Ưu tiên cải thiện các vấn đề được phản ánh nhiều như:", ", ".join([kw for kw, _ in top_neg_keywords[:3]]))
        else:
            print("  - 👍 Duy trì và phát huy các điểm tích cực:", ", ".join([kw for kw, _ in top_pos_keywords[:3]]))
    else:
        print("- Tổng số nhận xét đã phân tích:", len(df_filtered))
        print("- Trung bình tỷ lệ nhận xét tích cực:", sentiment_counts.get('positive', 0).round(2), "%")
        print("- Trung bình tỷ lệ nhận xét tiêu cực:", sentiment_counts.get('negative', 0).round(2), "%")
        print("- 📌 Gợi ý chung: Cải thiện feedback tiêu cực, phát huy điểm mạnh, theo dõi xu hướng sentiment định kỳ.")
