# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ed5LAIlqKOkULAp3sgcnkQXg9FEZPnGf

**Y√™u c·∫ßu 1:** C√°c c√¥ng ty
ƒëang nh·∫≠n nhi·ªÅu ƒë√°nh gi√°
(review) t·ª´ ·ª©ng vi√™n/nh√¢n
vi√™n ƒëƒÉng tr√™n ITViec ‚Üí
D·ª±a tr√™n nh·ªØng th√¥ng tin
n√†y ƒë·ªÉ ph√¢n t√≠ch c·∫£m x√∫c
(t√≠ch c·ª±c, ti√™u c·ª±c, trung
t√≠nh).

**Th∆∞ vi·ªán s·ª≠ d·ª•ng:**
* numpy, pandas, matplotlib, seaborn
* underthesea
* wordcloud
* scikit-learn (sklearn)
* ...

**B∆∞·ªõc 1: Business Understanding**

**M·ª•c ti√™u:** X√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n gi√∫p itviec
v√† c√°c c√¥ng ty ƒë·ªëi t√°c c√≥ th·ªÉ bi·∫øt ƒë∆∞·ª£c nh·ªØng ph·∫£n h·ªìi
nhanh ch√≥ng c·ªßa nh√¢n vi√™n/ ·ª©ng vi√™n v·ªÅ c√¥ng ty m√¨nh (t√≠ch
c·ª±c, ti√™u c·ª±c hay trung t√≠nh).

**B∆∞·ªõc 2: Data Understanding/ Acquire**

* D·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p s·∫µn trong c√°c t·∫≠p tin:
  * Overview_Companies.xls
  * Overview_Reviews.xls
  * Reviews.xls
* K√®m theo ƒë√≥ l√† file m√¥ t·∫£: M√¥ t·∫£ b·ªô d·ªØ li·ªáu ITViec.pdf

Sentiment analysis v·ªõi c√°c thu·∫≠t to√°n thu·ªôc nh√≥m
Supervised Learning ‚Äì Classification nh∆∞: Na√Øve
Bayes, KNN, Logistic Regression, Tree Algorithms,
SVM ...

**B∆∞·ªõc 3: Data preparation/ Prepare**

* chu·∫©n h√≥a text (ti·∫øng Vi·ªát, ti·∫øng Anh)
* x·ª≠ l√Ω l√†m s·∫°ch d·ªØ li·ªáu

**B∆∞·ªõc 4&5: Modeling & Evaluation/ Analyze & Report**

* X√¢y d·ª±ng c√°c Classification model cho Sentiment
Analysis (ch·ªçn √≠t nh·∫•t 3 thu·∫≠t to√°n)
* Th·ª±c hi·ªán/ ƒë√°nh gi√° k·∫øt qu·∫£ c√°c Classification
model: R-squared, ccc, precision, recall, f1, confusion matrix,
ROC curve...
* N·∫øu d·ªØ li·ªáu m·∫•t c√¢n b·∫±ng g√¢y ·∫£nh h∆∞·ªüng ƒë·∫øn k·∫øt qu·∫£ th√¨
xem x√©t th√™m vi·ªác x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng
* So s√°nh c√°c k·∫øt qu·∫£
* C√≥ th·ªÉ ƒë·ªÅ xu·∫•t th√™m c√°c thu·∫≠t to√°n m·ªõi

=> K·∫øt lu·∫≠n

**B∆∞·ªõc 6: Deployment & Feedback/ Act**

ƒê∆∞a ra nh·ªØng c·∫£i ti·∫øn ph√π h·ª£p ƒë·ªÉ n√¢ng cao s·ª±
h√†i l√≤ng c·ªßa nh√¢n vi√™n/ ·ª©ng vi√™n ‚Üí M√¥i tr∆∞·ªùng
l√†m vi·ªác t·ªët, thu h√∫t ·ª©ng vi√™n, gi·ªØ ch√¢n nh√¢n
vi√™n...

* Nh·∫≠n x√©t t√≠ch c·ª±c v√† ti√™u c·ª±c k√®m wordcloud c·ªßa t·ª´ng
lo·∫°i, c√°c keyword ch√≠nh li√™n quan, c√°c tr·ª±c quan v√†
th·ªëng k√™ c·∫ßn thi·∫øt...

  ‚Üí C√°c ph√¢n t√≠ch, ƒë·ªÅ xu·∫•t cho c√¥ng ty

# For Google Colab
"""

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt

def check_wordcloud(data, col_name):
  text = " ".join(data)
  wc = WordCloud(width=800, height=400, background_color='white').generate(text)
  plt.imshow(wc, interpolation='bilinear')
  plt.axis("off")
  plt.title("WordCloud c·ªßa " + col_name)
  plt.show()

"""## STEP 2: Clean Text (English + Vietnamese)"""

import re

def clean_text(text):
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt
    text = re.sub(r'[^\w\s\.,!?;:]', ' ', text)

    # X√≥a URL
    text = re.sub(r'http\S+|www\.\S+', '', text)

    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text)

    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi
    text = text.strip()

    return text

import regex
def normalize_repeated_characters(text):
    """
    Chu·∫©n h√≥a c√°c t·ª´ c√≥ k√Ω t·ª± l·∫∑p li√™n ti·∫øp
    V√≠ d·ª•: "l√≤nggggg" -> "l√≤ng", "thi·ªátttt" -> "thi·ªát"
    """
    # X·ª≠ l√Ω k√Ω t·ª± Vi·ªát Nam (bao g·ªìm d·∫•u)
    # Thay th·∫ø 3+ k√Ω t·ª± li√™n ti·∫øp b·∫±ng 1 k√Ω t·ª±
    text = re.sub(r'([aƒÉ√¢e√™iou√¥∆°∆∞y√†√°·∫£√£·∫°·∫±·∫Ø·∫≥·∫µ·∫∑·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç·ªì·ªë·ªï·ªó·ªô·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ])\1{2,}', r'\1', text, flags=re.IGNORECASE)

    # X·ª≠ l√Ω consonant
    text = re.sub(r'([bcdfghjklmnpqrstvwxz])\1{2,}', r'\1', text, flags=re.IGNORECASE)

    return text

import re

def normalize_punctuation(text):
    """Chu·∫©n h√≥a c√°c d·∫•u c√¢u"""

    # Chu·∫©n h√≥a d·∫•u ch·∫•m
    text = re.sub(r'\.{2,}', '.', text)

    # Chu·∫©n h√≥a d·∫•u h·ªèi ch·∫•m
    text = re.sub(r'\?{2,}', '?', text)

    # Chu·∫©n h√≥a d·∫•u c·∫£m th√°n
    text = re.sub(r'!{2,}', '!', text)

    # Chu·∫©n h√≥a d·∫•u ph·∫©y
    text = re.sub(r',{2,}', ',', text)

    # Lo·∫°i b·ªè d·∫•u nh√°y ƒë∆°n (n·∫øu c·∫ßn)
    text = text.replace("'", "")

    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng xung quanh d·∫•u c√¢u
    text = re.sub(r'\s*([.!?,:;])\s*', r'\1 ', text)
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

import unicodedata

def normalize_vietnamese(text):
    """Chu·∫©n h√≥a unicode ti·∫øng Vi·ªát s·ª≠ d·ª•ng unicodedata"""
    return unicodedata.normalize('NFC', text)

import html

def process_special_chars(text):
    # Decode HTML entities
    text = html.unescape(text)

    # X·ª≠ l√Ω emoji (t√πy task)
    # C√≥ th·ªÉ gi·ªØ nguy√™n ho·∫∑c thay th·∫ø b·∫±ng text m√¥ t·∫£

    return text

def process_basic_text(text, max_length=256):
    # 1. L√†m s·∫°ch c∆° b·∫£n
    text = clean_text(text)

    # 2. Chu·∫©n h√≥a k√Ω t·ª± l·∫∑p
    text = normalize_repeated_characters(text)

    # 3. Chu·∫©n h√≥a d·∫•u c√¢u
    text = normalize_punctuation(text)

    # 4. Chu·∫©n h√≥a ti·∫øng Vi·ªát
    text = normalize_vietnamese(text)

    # 5. X·ª≠ l√Ω k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = process_special_chars(text)

    return text


from pathlib import Path

# ƒê·ªçc t·ª´ kh√≥a t√≠ch c·ª±c
pos_words = set(Path("files/positive_words.txt").read_text("utf-8").splitlines())

# ƒê·ªçc t·ª´ kh√≥a ti√™u c·ª±c
neg_words = set(Path("files/negative_words.txt").read_text("utf-8").splitlines())

print(f"> Loaded {len(pos_words)} positive, {len(neg_words)} negative keywords.")

# ƒê·ªçc emoji t√≠ch c·ª±c
pos_emoji = set(Path("files/positive_emoji.txt").read_text("utf-8").splitlines())

# ƒê·ªçc emoji ti√™u c·ª±c
neg_emoji = set(Path("files/negative_emoji.txt").read_text("utf-8").splitlines())

print(f"Loaded {len(pos_emoji)} positive emojis, {len(neg_emoji)} negative emojis")

def calc_sentiment_features(text):
    '''
      pos_w, neg_w: s·ªë t·ª´ t√≠ch c·ª±c/ti√™u c·ª±c

      pos_e, neg_e: s·ªë emoji t√≠ch c·ª±c/ti√™u c·ª±c

      total_we: t·ªïng s·ªë t·ª´ v√† emoji mang c·∫£m x√∫c

      ratio_all: t·ª∑ l·ªá c√¢n b·∫±ng t√≠ch c·ª±c ‚Äì ti√™u c·ª±c
    '''

    toks = text.lower().strip()
    pos_w = sum(t in toks for t in pos_words)
    neg_w = sum(t in toks for t in neg_words)
    pos_e = sum(text.count(e) for e in pos_emoji)
    neg_e = sum(text.count(e) for e in neg_emoji)

    total_w = pos_w + neg_w
    total_e = pos_e + neg_e
    total_we = total_w + total_e

    ratio_words = (pos_w - neg_w) / total_w if total_w else 0
    ratio_emoji = (pos_e - neg_e) / total_e if total_e else 0
    ratio_all = (pos_w + pos_e - neg_w - neg_e) / total_we if total_we else 0

    return pos_w, neg_w, pos_e, neg_e, total_we, ratio_all


from langdetect import detect
def detect_lang_safe(text):
    # H√†m detect language ƒë∆°n gi·∫£n, b·∫°n c√≥ th·ªÉ d√πng langdetect ho·∫∑c rule ri√™ng
    try:
        return detect(text)
    except:
        return ''


import re
from collections import deque

def recursive_split_sentences(text, lang='vi'):
    """
    Recursively split sentences based on patterns for the specified language.

    Args:
        text (str): Input text to split
        lang (str): Language code ('vi' for Vietnamese, 'en' for English)

    Returns:
        list: List of split sentences
    """
    # Define patterns for each language
    if lang == 'vi':
        patterns = [
            r'\bm·ªói t·ªôi\b.*?\bdo\b.*?(?=,|\.|$)',
            r'\bdo\b.*?\bn√™n\b.*?(?=,|\.|$)',
            r'\bv√¨\b.*?\bn√™n\b.*?(?=,|\.|$)',
            r'\bm·∫∑c d√π\b.*?\bnh∆∞ng\b.*?(?=,|\.|$)',
            r'\bn·∫øu\b.*?\bth√¨\b.*?(?=,|\.|$)',
        ]
    elif lang == 'en':
        patterns = [
            r'\bif\b.*?\bthen\b.*?(?=,|\.|$)',
            r'\bwhen\b.*?\bthen\b.*?(?=,|\.|$)',
            r'\bsince\b.*?\btherefore\b.*?(?=,|\.|$)',
            r'\bbecause\b.*?\bso\b.*?(?=,|\.|$)',
            r'\bas\b.*?\bso\b.*?(?=,|\.|$)',
            r'\balthough\b.*?\byet\b.*?(?=,|\.|$)',
            r'\bthough\b.*?\bstill\b.*?(?=,|\.|$)',
            r'\bwhile\b.*?\bhowever\b.*?(?=,|\.|$)',
            r'\bunless\b.*?\botherwise\b.*?(?=,|\.|$)',
            r'\beven if\b.*?\bstill\b.*?(?=,|\.|$)',
        ]
    else:
        # Default to Vietnamese patterns
        patterns = [
            r'\bm·ªói t·ªôi\b.*?\bdo\b.*?(?=,|\.|$)',
            r'\bdo\b.*?\bn√™n\b.*?(?=,|\.|$)',
            r'\bv√¨\b.*?\bn√™n\b.*?(?=,|\.|$)',
            r'\bm·∫∑c d√π\b.*?\bnh∆∞ng\b.*?(?=,|\.|$)',
            r'\bn·∫øu\b.*?\bth√¨\b.*?(?=,|\.|$)',
        ]

    queue = deque([text.strip()])
    results = []

    while queue:
        sentence = queue.popleft().strip(" ,.")

        matched = False
        for pattern in patterns:
            match = re.search(pattern, sentence, re.IGNORECASE)
            if match:
                matched = True
                # Ph·∫ßn tr∆∞·ªõc m·∫´u
                before = sentence[:match.start()].strip(" ,.")
                # Ph·∫ßn tr√πng v·ªõi m·∫´u
                middle = match.group().strip(" ,.")
                # Ph·∫ßn sau m·∫´u
                after = sentence[match.end():].strip(" ,.")

                if before:
                    queue.append(before)
                results.append(middle + ".")  # ƒê∆∞a th·∫≥ng v√†o k·∫øt qu·∫£, kh√¥ng x·ª≠ l√Ω l·∫°i
                if after:
                    queue.append(after)
                break  # Ch·ªâ d√πng 1 pattern m·ªói v√≤ng

        if not matched:
            if sentence:
                results.append(sentence + ".")

    return results


def split_sentences_by_meaning(text, lang='vi'):
    """
    T√°ch c√¢u theo d·∫•u c√¢u v√† m·ªôt s·ªë li√™n t·ª´ chia √Ω.

    Args:
        text (str): Input text to split
        lang (str): Language code ('vi' for Vietnamese, 'en' for English)

    Returns:
        list: List of split sentences
    """
    # Danh s√°ch c√°c li√™n t·ª´ v√† t·ª´ n·ªëi ph·ªï bi·∫øn d√πng ƒë·ªÉ t√°ch √Ω
    if lang == 'vi':
        split_keywords = [
            r'\bnh∆∞ng\b', r'\btuy nhi√™n\b', r'\btuy\b', r'\bm·∫∑c d√π\b',
            r'\bd√π\b', r'\bv√¨ v·∫≠y\b', r'\bv√¨ th·∫ø\b', r'\bdo ƒë√≥\b',
            r'\bc≈©ng\b', r'\bsong\b',
        ]
    elif lang == 'en':
        split_keywords = [
            r'\bbut\b', r'\bhowever\b', r'\bnevertheless\b', r'\bnonetheless\b',
            r'\balthough\b', r'\bthough\b', r'\beven though\b', r'\bwhile\b',
            r'\bwhereas\b', r'\btherefore\b', r'\bthus\b', r'\bhence\b',
            r'\bconsequently\b', r'\bmoreover\b', r'\bfurthermore\b', r'\bin addition\b',
            r'\byet\b', r'\bso\b', r'\bfor\b', r'\bnor\b',
            r'\botherwise\b', r'\bmeanwhile\b', r'\bon the other hand\b',
            r'\bin contrast\b', r'\bsimilarly\b', r'\blikewise\b'
        ]
    else:
        # Default to Vietnamese keywords
        split_keywords = [
            r'\bnh∆∞ng\b', r'\btuy nhi√™n\b', r'\btuy\b', r'\bm·∫∑c d√π\b',
            r'\bd√π\b', r'\bv√¨ v·∫≠y\b', r'\bv√¨ th·∫ø\b', r'\bdo ƒë√≥\b',
            r'\bc≈©ng\b', r'\bsong\b',
        ]

    # B∆∞·ªõc 1: Chu·∫©n h√≥a vƒÉn b·∫£n
    text = text.strip()
    if not text:
        return []

    # B∆∞·ªõc 2: T√°ch c√¢u theo d·∫•u ng·∫Øt (., !, ?)
    sentences = re.split(r'(?<=[.!?])\s+', text)

    final_sentences = []

    # B∆∞·ªõc 3: V·ªõi m·ªói c√¢u, ti·∫øp t·ª•c t√°ch n·∫øu c√≥ li√™n t·ª´ mang nhi·ªÅu √Ω
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        # Gh√©p c√°c keyword l·∫°i th√†nh regex OR
        pattern = '|'.join(split_keywords)

        # T√°ch n·∫øu c√≥ t·ª´ n·ªëi nhi·ªÅu √Ω
        sub_sentences = re.split(pattern, sentence, flags=re.IGNORECASE)

        # Flatten the list of lists and strip each element
        for sub_sentence in sub_sentences:
            split_do_neu_result = recursive_split_sentences(sub_sentence, lang)
            final_sentences.extend([s.strip() for s in split_do_neu_result if s.strip()])

    return final_sentences


# Example usage
if __name__ == "__main__":
    # Test Vietnamese
    vietnamese_text = """
    V√¨ l√† c√¥ng ty IT service n√™n c√≥ nhi·ªÅu th·ª© ƒë·ªÉ trau d·ªìi v√† c·∫£i thi·ªán chuy√™n m√¥n R·∫•t √≠t khi OT, n·∫øu c√≥ OT th√¨ tr·∫£ l∆∞∆°ng OT ƒë·∫ßy ƒë·ªß theo lu·∫≠t
    """

    print("=== VIETNAMESE TEST ===")
    result_vi = split_sentences_by_meaning(vietnamese_text, lang='vi')
    for i, sentence in enumerate(result_vi, 1):
        print(f"{i}. {sentence}")

    # Test English
    english_text = """
    If you study hard then you will pass the exam, but if you don't study then you might fail.
    Although it was raining, we went to the park, however we got wet.
    """

    print("\n=== ENGLISH TEST ===")
    result_en = split_sentences_by_meaning(english_text, lang='en')
    for i, sentence in enumerate(result_en, 1):
        print(f"{i}. {sentence}")



import nltk
from nltk.corpus import stopwords

# T·∫£i b·ªô stopwords ti·∫øng Anh (ch·ªâ ch·∫°y 1 l·∫ßn)
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')

# H√†m ti·ªán √≠ch ƒë·ªÉ load t·∫•t c·∫£ d·ªØ li·ªáu
def load_processing_data():
    """Load t·∫•t c·∫£ d·ªØ li·ªáu c·∫ßn thi·∫øt cho vi·ªác x·ª≠ l√Ω vƒÉn b·∫£n"""

    # LOAD EMOJICON
    try:
        with open('files/emojicon.txt', 'r', encoding="utf8") as file:
            emoji_lst = file.read().split('\n')
        emoji_dict = {}
        for line in emoji_lst:
            if '\t' in line and line.strip():  # Ki·ªÉm tra format v√† kh√¥ng r·ªóng
                line = normalize_vietnamese(line)  # Chu·∫©n h√≥a unicode
                parts = line.split('\t', 1)  # Ch·ªâ split l·∫ßn ƒë·∫ßu
                if len(parts) == 2:
                    key, value = parts
                    emoji_dict[key] = str(value)
    except FileNotFoundError:
        print("Warning: emojicon.txt not found, using empty emoji dict")
        emoji_dict = {}

    # LOAD TEENCODE
    try:
        with open('files/teencode.txt', 'r', encoding="utf8") as file:
            teen_lst = file.read().split('\n')
        teen_dict = {}
        for line in teen_lst:
            if '\t' in line and line.strip():
                line = normalize_vietnamese(line)  # Chu·∫©n h√≥a unicode
                parts = line.split('\t', 1)
                if len(parts) == 2:
                    key, value = parts
                    teen_dict[key] = str(value)
    except FileNotFoundError:
        print("Warning: teencode.txt not found, using empty teen dict")
        teen_dict = {}

    #LOAD TRANSLATE ENGLISH -> VNMESE
    try:
        with open('files/english-vnmese.txt', 'r', encoding="utf8") as file:
            english_lst = file.read().split('\n')
        english_dict = {}
        for line in english_lst:
            if '\t' in line and line.strip():
                line = normalize_vietnamese(line)  # Chu·∫©n h√≥a unicode
                parts = line.split('\t', 1)
                if len(parts) == 2:
                    key, value = parts
                    english_dict[key] = str(value)
    except FileNotFoundError:
        print("Warning: teencode.txt not found, using empty teen dict")
        english_dict = {}

    # LOAD WRONG WORDS
    try:
        with open('files/wrong-word.txt', 'r', encoding="utf8") as file:
            wrong_lst = file.read().split('\n')
        wrong_lst = [normalize_vietnamese(word.strip()) for word in wrong_lst if word.strip()]  # Chu·∫©n h√≥a unicode
    except FileNotFoundError:
        print("Warning: wrong-word.txt not found, using empty wrong list")
        wrong_lst = []


    # LOAD STOPWORDS
    try:
        with open('files/vietnamese-stopwords.txt', 'r', encoding="utf8") as file:
            stopwords_vi = file.read().split('\n')
        stopwords_vi = [normalize_vietnamese(word.strip()) for word in stopwords_vi if word.strip()]  # Chu·∫©n h√≥a unicode
        # Lo·∫°i b·ªè c√°c t·ª´ ph·ªß ƒë·ªãnh trong stopword
        negations_vi = {'kh√¥ng', 'ch·∫≥ng', 'ch·∫£', 'ƒë√¢u', 'ch∆∞a', 'ƒë·ª´ng', 'kh·ªèi'}
        stopwords_vi = [word for word in stopwords_vi if word not in negations_vi]
        # Lo·∫£i b·ªè c√°c t·ª´ quan tr·ªçng
        keep_words = {'r·∫•t', 'kh√¥ng', 'n√™n', 'c√≥', 't·ªët', 'x·∫•u', 'l√†m vi·ªác', '√°p l·ª±c', 'tho·∫£i m√°i', '·ªïn', 'h√†i l√≤ng', 'kh√≥', 't·ªá', 'c·ª±c k·ª≥', 'nhi·ªÅu', '√≠t', 'cao', 'th·∫•p', 'kh√°'}
        stopwords_vi = [word for word in stopwords_vi if word not in keep_words]
    except FileNotFoundError:
        print("Warning: vietnamese-stopwords.txt not found, using empty stopwords list")
        stopwords_vi = []

    # L·∫•y b·ªô stopwords ti·∫øng Anh
    stopwords_en = set(stopwords.words('english'))

    # Lo·∫°i b·ªè t·ª´ ph·ªß ƒë·ªãnh kh·ªèi b·ªô stopwords
    negations = {'not', 'no', 'nor', 'don', 'didn', 'doesn', "don't", "didn't", "doesn't"}
    stopwords_en = [word for word in stopwords_en if word not in negations]
    # Lo·∫°i b·ªè c√°c t·ª´ quan tr·ªçng
    keep_words = {'not', 'very', 'no', 'never', 'good', 'bad', 'great', 'poor', 'excellent', 'happy', 'sad', 'stressful', 'comfortable', 'satisfied', 'unhappy', 'awful', 'amazing'}
    stopwords_en = [word for word in stopwords_en if word not in keep_words]

    return emoji_dict, teen_dict, english_dict, wrong_lst, stopwords_vi, stopwords_en

emoji_dict, teen_dict, english_dict, wrong_lst, stopwords_vi, stopwords_en = load_processing_data()

import re
from underthesea import pos_tag as vi_pos_tag
from underthesea import word_tokenize as vi_tokenize
from nltk import pos_tag as en_pos_tag
from nltk.tokenize import word_tokenize as en_tokenize

# POS gi·ªØ l·∫°i
VI_POS_KEEP = {'N', 'V', 'A', 'R'}
EN_POS_KEEP = {'NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS'}

# T·ª´ ng·∫Øn quan tr·ªçng (negations, intensifiers)
VI_KEEP_SHORT = {'kh√¥ng', 'r·∫•t', 'qu√°', 'h∆°i', '√≠t', 'cao', 'th·∫•p', 'kh√°'}
EN_KEEP_SHORT = {'not', 'no', 'very', 'too'}

def normalize_text(text, dict_map):
    # Thay th·∫ø t·ª´ theo dict_map (teencode, sai ch√≠nh t·∫£, emoji...)
    pattern = re.compile(r'\b(' + '|'.join(re.escape(key) for key in dict_map.keys()) + r')\b', flags=re.IGNORECASE)
    def replace_func(match):
        key = match.group(0).lower()
        return dict_map.get(key, key)
    return pattern.sub(replace_func, text)

def replace_emoji(text, emoji_dict):
    # Thay emoji t·ª´ng k√Ω t·ª±
    for emo, rep in emoji_dict.items():
        text = text.replace(emo, ' ' + rep + ' ')
    return text

def remove_wrong_words(text, wrong_lst):
    if not text:
        return ""
    words = text.split()
    cleaned_words = [word for word in words if word not in wrong_lst]
    return " ".join(cleaned_words)

def process_text(text):
    if not text or not text.strip():
        return ""

    lang = detect_lang_safe(text)

    # B∆∞·ªõc 1: Thay emoji th√†nh t·ª´ t∆∞∆°ng ·ª©ng
    text = replace_emoji(text, emoji_dict)

    # B∆∞·ªõc 2: Chu·∫©n h√≥a teencode
    text = normalize_text(text, teen_dict)

    # B∆∞·ªõc 3: Chu·∫©n h√≥a t·ª´ sai ch√≠nh t·∫£
    if lang == 'vi':
        text = remove_wrong_words(text, wrong_lst)
    else:
        text = normalize_text(text, english_dict)

    if lang == 'vi':
        tokens = vi_tokenize(text, format="text")
        tagged = vi_pos_tag(tokens)
        # L·ªçc t·ª´ theo POS v√† t·ª´ ng·∫Øn quan tr·ªçng
        results = [word for word, pos in tagged if (pos in VI_POS_KEEP or word.lower() in VI_KEEP_SHORT)]
        # Lo·∫°i b·ªè stopword
        results = [w for w in results if w.lower() not in stopwords_vi]
    else:
        tokens = en_tokenize(text)
        tagged = en_pos_tag(tokens)
        results = [word for word, pos in tagged if (pos in EN_POS_KEEP or word.lower() in EN_KEEP_SHORT)]
        results = [w for w in results if w.lower() not in stopwords_en]

    return ' '.join(results)

# V√≠ d·ª• ch·∫°y th·ª≠
if __name__ == '__main__':
    vi_example = "M√¥i tr∆∞·ªùng l√†m vi·ªác r·∫•t t·ªët üòä, nh∆∞ng √°p l·ª±c hok nh·ªè v√† l∆∞∆°ng th·∫•p."
    en_example = "The working environment is very good üòä, but the pressure is high and the salary is low."

    print("Vietnamese:", process_text(vi_example))
    print("English:", process_text(en_example))

import re
from underthesea import pos_tag as vi_pos_tag
from underthesea import word_tokenize as vi_tokenize
from nltk import pos_tag as en_pos_tag
from nltk.tokenize import word_tokenize as en_tokenize

def process_tagged_sentence(tagged_words, language='vi'):
    """
    X·ª≠ l√Ω c√¢u ƒë√£ ƒë∆∞·ª£c g√°n nh√£n t·ª´ lo·∫°i ƒë·ªÉ t·∫°o c√¢u c√≥ nghƒ©a v·ªõi ph√¢n c·ª•m √Ω nghƒ©a
    H·ªó tr·ª£ c·∫£ ti·∫øng Vi·ªát v√† ti·∫øng Anh

    Args:
        tagged_words: Danh s√°ch c√°c tuple (t·ª´, nh√£n_t·ª´_lo·∫°i)
        language: 'vi' cho ti·∫øng Vi·ªát, 'en' cho ti·∫øng Anh

    Returns:
        str: C√¢u ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω, ph√¢n th√†nh c√°c c·ª•m √Ω nghƒ©a
    """

    # ƒê·ªãnh nghƒ©a c√°c t·ª´ lo·∫°i c·∫ßn gi·ªØ l·∫°i (c√≥ nghƒ©a) cho c·∫£ ti·∫øng Vi·ªát v√† ti·∫øng Anh
    meaningful_tags = {
        'N', 'NN', 'NNS', 'NNP', 'NNPS',  # Danh t·ª´ (Noun)
        'V', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # ƒê·ªông t·ª´ (Verb)
        'A', 'JJ', 'JJR', 'JJS',  # T√≠nh t·ª´ (Adjective)
    }

    # ƒê·ªãnh nghƒ©a c√°c t·ª´ c·ª• th·ªÉ c·∫ßn lo·∫°i b·ªè theo ng√¥n ng·ªØ
    if language == 'vi':
        stop_words = {
            'c√≥_th·ªÉ', '·ªü', 't·ª´', 'v√¨', 'm·ªói', 'c·ªßa', 'v·ªõi', 'theo', 'trong', 'tr√™n', 'd∆∞·ªõi'
        }
        # Tr·∫°ng t·ª´ quan tr·ªçng c·∫ßn gi·ªØ
        important_adverbs = {'kh√¥ng', 'kh√°', 'r·∫•t', 't∆∞∆°ng_ƒë·ªëi', 'kh√°_l√†', 'ƒë√£', 's·∫Ω', 'ƒëang'}
        # T·ª´ quan tr·ªçng c·∫ßn gi·ªØ b·∫•t k·ªÉ t·ª´ lo·∫°i
        important_words = {'n√™n', 'v√†', 'ho·∫∑c', 'c≈©ng', 'th√¨', 'l√†', 'ƒë∆∞·ª£c'}
        # T·ª´ lo·∫°i d·∫•u c√¢u v√† gi·ªõi t·ª´ c·∫ßn b·ªè qua
        skip_tags = {'CH', 'E', 'C'}
    else:  # English
        stop_words = {
            'of', 'in', 'on', 'at', 'by', 'for', 'with', 'from', 'to', 'about',
            'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between'
        }
        # Tr·∫°ng t·ª´ quan tr·ªçng c·∫ßn gi·ªØ
        important_adverbs = {'not', 'very', 'quite', 'really', 'too', 'so', 'already', 'still', 'just'}
        # T·ª´ quan tr·ªçng c·∫ßn gi·ªØ b·∫•t k·ªÉ t·ª´ lo·∫°i
        important_words = {'and', 'or', 'but', 'should', 'must', 'can', 'will', 'would', 'could', 'is', 'are', 'was', 'were', 'be', 'been', 'being'}
        # T·ª´ lo·∫°i d·∫•u c√¢u v√† gi·ªõi t·ª´ c·∫ßn b·ªè qua
        skip_tags = {'.', ',', ':', ';', '!', '?', 'IN', 'TO', 'CC'}

    meaningful_words = []
    current_chunk = []
    check_tags = set()

    for i, (word, tag) in enumerate(tagged_words):
        # B·ªè qua c√°c t·ª´ trong danh s√°ch stop words
        if word.lower() in stop_words:
            continue

        # X·ª≠ l√Ω d·∫•u ph·∫©y - k·∫øt th√∫c c·ª•m hi·ªán t·∫°i
        if word == ',' and current_chunk:
            if len(check_tags) == 1 and meaningful_words:
                last_text = meaningful_words.pop()
                meaningful_words.append(last_text + ', ' + ' '.join(current_chunk))
            else:
                meaningful_words.append(' '.join(current_chunk))
            current_chunk = []
            check_tags = set()
            continue

        # B·ªè qua d·∫•u ch·∫•m cu·ªëi c√¢u
        if word == '.' and i == len(tagged_words) - 1:
            continue

        # Gi·ªØ l·∫°i t·ª´ quan tr·ªçng b·∫•t k·ªÉ t·ª´ lo·∫°i
        if word.lower() in important_words:
            current_chunk.append(word)
            check_tags.add(tag)
            continue

        # Gi·ªØ l·∫°i c√°c t·ª´ c√≥ nghƒ©a
        if tag in meaningful_tags:
            # Ki·ªÉm tra tr·∫°ng t·ª´ quan tr·ªçng
            if tag in {'R', 'RB', 'RBR', 'RBS'} and word.lower() not in important_adverbs:
                continue
            current_chunk.append(word)
            check_tags.add(tag)
        elif tag not in skip_tags:  # Gi·ªØ l·∫°i c√°c tag kh√°c n·∫øu kh√¥ng ph·∫£i d·∫•u c√¢u, gi·ªõi t·ª´, li√™n t·ª´
            current_chunk.append(word)
            check_tags.add(tag)

    # Th√™m c·ª•m cu·ªëi c√πng n·∫øu c√≤n
    if current_chunk:
        if len(check_tags) == 1 and meaningful_words:
              last_text = meaningful_words.pop()
              meaningful_words.append(last_text + ', ' + ' '.join(current_chunk))
        else:
            meaningful_words.append(' '.join(current_chunk))

    # H√†m vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu
    def capitalize_first_word(text):
        if not text:
            return text
        words = text.split()
        if words and len(words[0]) > 0:
            first_word = words[0]
            words[0] = first_word[0].upper() + first_word[1:]
        return ' '.join(words)

    # T·∫°o c√¢u v·ªõi ƒë·ªãnh d·∫°ng mong mu·ªën v√† vi·∫øt hoa
    if len(meaningful_words) == 1:
        return capitalize_first_word(meaningful_words[0]) + "."
    elif len(meaningful_words) == 2:
        return (capitalize_first_word(meaningful_words[0]) + ". " +
                capitalize_first_word(meaningful_words[1]) + ".")
    elif len(meaningful_words) >= 3:
        # N·ªëi 2 c·ª•m ƒë·∫ßu b·∫±ng d·∫•u ph·∫©y, c√°c c·ª•m sau b·∫±ng d·∫•u ch·∫•m
        result = capitalize_first_word(meaningful_words[0]) + ", " + capitalize_first_word(meaningful_words[1]) + ". "
        for chunk in meaningful_words[2:]:
            result += capitalize_first_word(chunk) + ". "
        return result.rstrip()

    return ""


def batch_process_sentences(sentences_data, language='vi'):
    """
    X·ª≠ l√Ω nhi·ªÅu c√¢u c√πng l√∫c cho c·∫£ ti·∫øng Vi·ªát v√† ti·∫øng Anh

    Args:
        sentences_data: Danh s√°ch c√°c c√¢u ƒë√£ ƒë∆∞·ª£c tag
        language: 'vi' cho ti·∫øng Vi·ªát, 'en' cho ti·∫øng Anh
    """
    results = []

    for tagged_words in sentences_data:
        processed = process_tagged_sentence(tagged_words, language)
        results.append(processed)

    return results


def advanced_process_with_context(tagged_words, keep_context_words=False, language='vi'):
    """
    X·ª≠ l√Ω n√¢ng cao v·ªõi t√πy ch·ªçn gi·ªØ l·∫°i m·ªôt s·ªë t·ª´ ng·ªØ c·∫£nh
    H·ªó tr·ª£ c·∫£ ti·∫øng Vi·ªát v√† ti·∫øng Anh

    Args:
        tagged_words: Danh s√°ch c√°c tuple (t·ª´, nh√£n_t·ª´_lo·∫°i)
        keep_context_words: C√≥ gi·ªØ l·∫°i m·ªôt s·ªë t·ª´ ng·ªØ c·∫£nh kh√¥ng
        language: 'vi' cho ti·∫øng Vi·ªát, 'en' cho ti·∫øng Anh

    Returns:
        str: C√¢u ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
    """

    # T·ª´ lo·∫°i ch√≠nh c·∫ßn gi·ªØ cho c·∫£ ti·∫øng Vi·ªát v√† ti·∫øng Anh
    core_tags = {
        'N', 'NN', 'NNS', 'NNP', 'NNPS',  # Danh t·ª´
        'V', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # ƒê·ªông t·ª´
        'A', 'JJ', 'JJR', 'JJS'  # T√≠nh t·ª´
    }

    # T·ª´ lo·∫°i ph·ª• c√≥ th·ªÉ gi·ªØ t√πy theo ng·ªØ c·∫£nh
    context_tags = {'R', 'L', 'RB', 'RBR', 'RBS', 'DT', 'PDT', 'WDT'} if keep_context_words else set()

    # T·ª´ quan tr·ªçng c·∫ßn gi·ªØ b·∫•t k·ªÉ t·ª´ lo·∫°i theo ng√¥n ng·ªØ
    if language == 'vi':
        important_words = {
            'kh√¥ng', 'kh√°', 't·ªët', 'cao', 'th·∫•p', 'nhi·ªÅu', '√≠t', 'n√™n', 'r·∫•t', 'ƒë√£', 's·∫Ω', 'ƒë∆∞·ª£c'
        }
    else:  # English
        important_words = {
            'not', 'quite', 'good', 'high', 'low', 'many', 'few', 'should', 'very', 'already', 'will', 'would'
        }

    meaningful_words = []

    for word, tag in tagged_words:
        # Lu√¥n gi·ªØ t·ª´ quan tr·ªçng
        if word.lower() in important_words:
            meaningful_words.append(word)
            continue

        # Gi·ªØ t·ª´ lo·∫°i ch√≠nh
        if tag in core_tags:
            meaningful_words.append(word)
            continue

        # Gi·ªØ t·ª´ lo·∫°i ng·ªØ c·∫£nh n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if tag in context_tags:
            meaningful_words.append(word)
            continue

    return ' '.join(meaningful_words)


def detect_language(tagged_words):
    """
    T·ª± ƒë·ªông ph√°t hi·ªán ng√¥n ng·ªØ d·ª±a tr√™n POS tags

    Args:
        tagged_words: Danh s√°ch c√°c tuple (t·ª´, nh√£n_t·ª´_lo·∫°i)

    Returns:
        str: 'vi' cho ti·∫øng Vi·ªát, 'en' cho ti·∫øng Anh
    """
    english_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS'}
    vietnamese_tags = {'N', 'V', 'A', 'R', 'L', 'CH', 'E', 'C'}

    tags_in_sentence = {tag for _, tag in tagged_words}

    english_count = len(tags_in_sentence.intersection(english_tags))
    vietnamese_count = len(tags_in_sentence.intersection(vietnamese_tags))

    return 'en' if english_count > vietnamese_count else 'vi'


def auto_process_tagged_sentence(tagged_words, keep_context_words=False):
    """
    T·ª± ƒë·ªông ph√°t hi·ªán ng√¥n ng·ªØ v√† x·ª≠ l√Ω c√¢u

    Args:
        tagged_words: Danh s√°ch c√°c tuple (t·ª´, nh√£n_t·ª´_lo·∫°i)
        keep_context_words: C√≥ gi·ªØ l·∫°i m·ªôt s·ªë t·ª´ ng·ªØ c·∫£nh kh√¥ng

    Returns:
        str: C√¢u ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
    """
    language = detect_language(tagged_words)
    return process_tagged_sentence(tagged_words, language)


# V√≠ d·ª• s·ª≠ d·ª•ng
if __name__ == "__main__":
    # Test v·ªõi d·ªØ li·ªáu m·∫´u ti·∫øng Vi·ªát
    sentence1_vi = [('C·∫ßn', 'V'), ('ƒë√°nh_gi√°', 'V'), ('KPI kh√°ch_quan', 'N'), ('v√†', 'C'), ('ph√¢n_chia', 'V'), ('c√¥ng_vi·ªác', 'V'), ('chuy√™n_nghi·ªáp', 'N')
                 , (',', 'CH'), ('match', 'V'), ('v·ªõi', 'E'), ('kh·∫£_nƒÉng', 'N'), (',', 'CH'), ('tr√¨nh_ƒë·ªô', 'N'), ('c·ªßa', 'E'), ('nh√¢n_vi√™n', 'N'), ('.', 'CH')]

    sentence2_vi = [('M√¥i_tr∆∞·ªùng', 'N'), ('tho·∫£i_m√°i', 'N'), (',', 'CH'), ('√≠t', 'A'), ('√°p_l·ª±c', 'N'), (',', 'CH'), ('c√≥_th·ªÉ', 'V'), ('l√†m_vi·ªác hybrid', 'N'), (',', 'CH')
                , ('l∆∞∆°ng', 'N'), ('n√™n', 'C'), ('deal', 'V'), ('t·ªët', 'A'), ('t·ª´', 'E'), ('l√∫c', 'N'), ('ƒë·∫ßu', 'N'), (',', 'CH'), ('v√¨', 'E'), ('m·ª©c', 'N'), ('tƒÉng', 'V')
                , ('m·ªói', 'L'), ('nƒÉm', 'N'), ('kh√¥ng', 'R'), ('cao', 'A'), ('.', 'CH')]

    # Test v·ªõi d·ªØ li·ªáu m·∫´u ti·∫øng Anh
    sentence1_en = [('Need', 'VB'), ('to', 'TO'), ('evaluate', 'VB'), ('objective', 'JJ'), ('KPI', 'NN'), ('and', 'CC'), ('divide', 'VB'), ('work', 'NN'), ('professionally', 'RB'),
                    (',', ','), ('match', 'VB'), ('with', 'IN'), ('ability', 'NN'), (',', ','), ('level', 'NN'), ('of', 'IN'), ('employee', 'NN'), ('.', '.')]

    sentence2_en = [('Comfortable', 'JJ'), ('environment', 'NN'), (',', ','), ('less', 'JJR'), ('pressure', 'NN'), (',', ','), ('can', 'MD'), ('work', 'VB'), ('hybrid', 'JJ'), (',', ','),
                    ('salary', 'NN'), ('should', 'MD'), ('deal', 'VB'), ('good', 'JJ'), ('from', 'IN'), ('beginning', 'NN'), (',', ','), ('because', 'IN'), ('increase', 'NN'),
                    ('each', 'DT'), ('year', 'NN'), ('not', 'RB'), ('high', 'JJ'), ('.', '.')]

    # Test x·ª≠ l√Ω c√¢u ƒë∆°n l·∫ª ti·∫øng Vi·ªát
    print("=== Test x·ª≠ l√Ω c√¢u ƒë∆°n l·∫ª ti·∫øng Vi·ªát ===")
    result1_vi = process_tagged_sentence(sentence1_vi, 'vi')
    print(f"C√¢u 1 VI: {result1_vi}")

    result2_vi = process_tagged_sentence(sentence2_vi, 'vi')
    print(f"C√¢u 2 VI: {result2_vi}")

    # Test x·ª≠ l√Ω c√¢u ƒë∆°n l·∫ª ti·∫øng Anh
    print("\n=== Test x·ª≠ l√Ω c√¢u ƒë∆°n l·∫ª ti·∫øng Anh ===")
    result1_en = process_tagged_sentence(sentence1_en, 'en')
    print(f"C√¢u 1 EN: {result1_en}")

    result2_en = process_tagged_sentence(sentence2_en, 'en')
    print(f"C√¢u 2 EN: {result2_en}")

    # Test t·ª± ƒë·ªông ph√°t hi·ªán ng√¥n ng·ªØ
    print("\n=== Test t·ª± ƒë·ªông ph√°t hi·ªán ng√¥n ng·ªØ ===")
    auto_result1_vi = auto_process_tagged_sentence(sentence1_vi)
    print(f"T·ª± ƒë·ªông VI: {auto_result1_vi}")

    auto_result1_en = auto_process_tagged_sentence(sentence1_en)
    print(f"T·ª± ƒë·ªông EN: {auto_result1_en}")

    # Test x·ª≠ l√Ω batch
    print("\n=== Test x·ª≠ l√Ω batch ===")
    sentences_data_vi = [sentence1_vi, sentence2_vi]
    sentences_data_en = [sentence1_en, sentence2_en]

    batch_results_vi = batch_process_sentences(sentences_data_vi, 'vi')
    print("Batch VI:")
    for result in batch_results_vi:
        print(f"  {result}")

    batch_results_en = batch_process_sentences(sentences_data_en, 'en')
    print("Batch EN:")
    for result in batch_results_en:
        print(f"  {result}")

    # Test x·ª≠ l√Ω n√¢ng cao
    print("\n=== Test x·ª≠ l√Ω n√¢ng cao ===")
    advanced_result1_vi = advanced_process_with_context(sentence1_vi, keep_context_words=True, language='vi')
    print(f"C√¢u 1 VI (gi·ªØ ng·ªØ c·∫£nh): {advanced_result1_vi}")

    advanced_result2_en = advanced_process_with_context(sentence2_en, keep_context_words=False, language='en')

def normalize_text(text, dict_map):
    # Thay th·∫ø t·ª´ theo dict_map (teencode, sai ch√≠nh t·∫£, emoji...)
    pattern = re.compile(r'\b(' + '|'.join(re.escape(key) for key in dict_map.keys()) + r')\b', flags=re.IGNORECASE)
    def replace_func(match):
        key = match.group(0).lower()
        return dict_map.get(key, key)
    return pattern.sub(replace_func, text)

def replace_emoji(text, emoji_dict):
    # Thay emoji t·ª´ng k√Ω t·ª±
    for emo, rep in emoji_dict.items():
        text = text.replace(emo, ' ' + rep + ' ')
    return text

def process_text(text, lang='vi'):
    if not text or not text.strip():
        return ""

    # B∆∞·ªõc 1: Thay emoji th√†nh t·ª´ t∆∞∆°ng ·ª©ng
    text = replace_emoji(text, emoji_dict)

    # B∆∞·ªõc 2: Chu·∫©n h√≥a teencode
    text = normalize_text(text, teen_dict)

    if lang == 'vi':
        tokens = vi_tokenize(text, format="text")
        tagged = vi_pos_tag(tokens)
        results = process_tagged_sentence(tagged).split('.')
        results = [s.strip() for s in results if s.strip()]
    else:
        tokens = en_tokenize(text)
        tagged = en_pos_tag(tokens)
        results = process_tagged_sentence(tagged).split('.')
        results = [s.strip() for s in results if s.strip()]

    return results

def process_split_text(split_text, lang='vi'):
    """
    X·ª≠ l√Ω nhi·ªÅu c√¢u c√πng l√∫c

    """
    results = []
    for text in split_text:
        if not text or not text.strip():
            continue

        results.extend(process_text(text, lang))
    # lo·∫°i b·ªè c√¢u tr√πng
    results = list(set(results))
    # n·ªëi th√†nh 1 chu·ªói
    return '. '.join(results)

# Oversampling n√¢ng cao
from collections import Counter

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Define models
models = {
    "Naive Bayes": MultinomialNB(),
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "SVM": SVC(kernel='linear', probability=True, class_weight='balanced'),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
}

import seaborn as sns
def analyze_reviews(df, company_name=None, col_pred='Pred_FN'):
    """
    Ph√¢n t√≠ch sentiment t·ª´ nh·∫≠n x√©t ·ª©ng vi√™n/nh√¢n vi√™n.
    """
    if company_name:
        print(f"\nüìå C√¥ng ty: {company_name}")
        df_filtered = df[df['Company Name'] == company_name].copy()
        if df_filtered.empty:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu.")
            return
    else:
        print("\nüìä Ph√¢n t√≠ch to√†n b·ªô d·ªØ li·ªáu")
        df_filtered = df.copy()
        if df_filtered.empty:
            print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu.")
            return

    # 1. Th·ªëng k√™ t·ªïng quan sentiment
    print("\nüìà Th·ªëng k√™ c·∫£m x√∫c:")
    sentiment_counts = df_filtered[col_pred].value_counts(normalize=True).reindex(['positive', 'neutral', 'negative'], fill_value=0) * 100
    sentiment_df = sentiment_counts.to_frame().T.rename(index={0: "T·ª∑ l·ªá (%)"})
    print(sentiment_df.style.format("{:.2f}%"))

    plt.figure(figsize=(8, 5))
    sns.countplot(data=df_filtered, x=col_pred, order=['positive', 'neutral', 'negative'], palette='viridis')
    plt.title("Ph√¢n b·ªë c·∫£m x√∫c nh·∫≠n x√©t")
    plt.xlabel("C·∫£m x√∫c")
    plt.ylabel("S·ªë l∆∞·ª£ng")
    plt.tight_layout()
    plt.show()

    # 2. X·ª≠ l√Ω c√°c nh·∫≠n x√©t t√≠ch c·ª±c/ti√™u c·ª±c
    def process_sentiment_group(label, color, word_list):
        reviews = df_filtered[df_filtered[col_pred] == label]['clean_advance_text2'].dropna()
        print(f"\n{'‚ú®' if label == 'positive' else 'üëé'} Nh·∫≠n x√©t {label}: {len(reviews)} nh·∫≠n x√©t")
        if reviews.empty:
            print("  - Kh√¥ng c√≥ nh·∫≠n x√©t n√†o.")
            return [], []

        text_combined = " ".join(reviews)
        try:
            stop_word = stopwords_vi + stopwords_en
            wordcloud = WordCloud(
                width=400, height=200, background_color='white',
                stopwords=stop_word, min_font_size=10
            ).generate(text_combined)
            print("  - Wordcloud:")
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis("off")
            plt.tight_layout()
            plt.show()
        except Exception as e:
            print(f"  - ‚ùå L·ªói t·∫°o Wordcloud: {e}")

        all_keyword_counts = Counter()

        for word in word_list:
            if word.strip():
                word_lower = word.lower()
                count = text_combined.replace('_', ' ').lower().count(word_lower)
                if count > 0:
                  all_keyword_counts[word] = count

        common_keywords = all_keyword_counts.most_common(10)

        print("  - Top keywords:")
        print(pd.DataFrame(common_keywords, columns=['Keyword', 'T·∫ßn su·∫•t']))
        return reviews, common_keywords

    pos_reviews, top_pos_keywords = process_sentiment_group("positive", "green", pos_words)
    neg_reviews, top_neg_keywords = process_sentiment_group("negative", "red", neg_words)

    # 3. Ph√¢n t√≠ch theo c√¥ng ty (ch·ªâ khi ph√¢n t√≠ch to√†n b·ªô)
    if company_name is None:
        print("\nüè¢ Th·ªëng k√™ theo C√¥ng ty:")
        top_company_counts = df_filtered['Company Name'].value_counts().head(10)
        print(top_company_counts.to_frame(name='S·ªë nh·∫≠n x√©t'))

        avg_sentiment = df_filtered.groupby('Company Name')['sentiment_ratio'].mean().sort_values(ascending=False)
        print("\nTop 10 C√¥ng ty c√≥ Sentiment Ratio cao nh·∫•t:")
        print(avg_sentiment.head(10).to_frame(name='Trung b√¨nh Sentiment Ratio'))
        print("Top 10 C√¥ng ty c√≥ Sentiment Ratio th·∫•p nh·∫•t:")
        print(avg_sentiment.tail(10).to_frame(name='Trung b√¨nh Sentiment Ratio'))

        top_bottom = avg_sentiment.head(5).index.tolist() + avg_sentiment.tail(5).index.tolist()
        df_top_bottom = df_filtered[df_filtered['Company Name'].isin(top_bottom)]

        plt.figure(figsize=(12, 6))
        sns.violinplot(data=df_top_bottom, x='sentiment_ratio', y='Company Name', palette='viridis')
        plt.title('Ph√¢n b·ªë Sentiment Ratio theo C√¥ng ty')
        plt.xlabel('Sentiment Ratio')
        plt.ylabel('C√¥ng ty')
        plt.tight_layout()
        plt.show()

    # 4. G·ª£i √Ω & ƒë·ªÅ xu·∫•t
    print("\nüí° ƒê·ªÅ xu·∫•t:")
    if company_name:
        print(f"- T·ªïng nh·∫≠n x√©t: {len(df_filtered)}")
        print(f"- T√≠ch c·ª±c: {sentiment_counts.get('positive', 0):.2f}%")
        print(f"- Ti√™u c·ª±c: {sentiment_counts.get('negative', 0):.2f}%")

        if top_pos_keywords:
            print("  - ƒêi·ªÉm m·∫°nh:", ", ".join([kw for kw, _ in top_pos_keywords[:5]]))
        if top_neg_keywords:
            print("  - C·∫ßn c·∫£i thi·ªán:", ", ".join([kw for kw, _ in top_neg_keywords[:5]]))

        if sentiment_counts.get('negative', 0) > sentiment_counts.get('positive', 0):
            print("  - üëâ ∆Øu ti√™n c·∫£i thi·ªán c√°c v·∫•n ƒë·ªÅ ƒë∆∞·ª£c ph·∫£n √°nh nhi·ªÅu nh∆∞:", ", ".join([kw for kw, _ in top_neg_keywords[:3]]))
        else:
            print("  - üëç Duy tr√¨ v√† ph√°t huy c√°c ƒëi·ªÉm t√≠ch c·ª±c:", ", ".join([kw for kw, _ in top_pos_keywords[:3]]))
    else:
        print("- T·ªïng s·ªë nh·∫≠n x√©t ƒë√£ ph√¢n t√≠ch:", len(df_filtered))
        print("- Trung b√¨nh t·ª∑ l·ªá nh·∫≠n x√©t t√≠ch c·ª±c:", sentiment_counts.get('positive', 0).round(2), "%")
        print("- Trung b√¨nh t·ª∑ l·ªá nh·∫≠n x√©t ti√™u c·ª±c:", sentiment_counts.get('negative', 0).round(2), "%")
        print("- üìå G·ª£i √Ω chung: C·∫£i thi·ªán feedback ti√™u c·ª±c, ph√°t huy ƒëi·ªÉm m·∫°nh, theo d√µi xu h∆∞·ªõng sentiment ƒë·ªãnh k·ª≥.")
