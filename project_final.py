# -*- coding: utf-8 -*-
print("üöÄ project_final.py starting import")
# Th∆∞ vi·ªán chu·∫©n
import re
import warnings
import unicodedata
import html
from tqdm import tqdm
tqdm.pandas()
from langdetect import detect

# X·ª≠ l√Ω d·ªØ li·ªáu v√† NLP
import pandas as pd
import numpy as np
from underthesea import word_tokenize as vi_tokenize
import nltk
from nltk.corpus import stopwords
# T·∫£i b·ªô stopwords ti·∫øng Anh
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')

from nltk.tokenize import word_tokenize as en_tokenize
from underthesea import pos_tag as vi_pos_tag
from nltk import pos_tag as en_pos_tag

# Tr·ª±c quan h√≥a
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Machine Learning
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score, davies_bouldin_score
)

# M√¥ h√¨nh h·ªçc m√°y
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering

import yake
from summa import keywords as textrank_keywords

# L∆∞u m√¥ h√¨nh
import joblib
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity
import gdown
import os
from sentence_transformers import SentenceTransformer

if not os.path.exists("clustering/sentence_bert.pkl"):
    gdown.download("https://drive.google.com/uc?id=1H7_KROPikN6ru4lccn7H7b3Iacbw6-xU", "clustering/sentence_bert.pkl", quiet=False)

if not os.path.exists("sentiment/stacking.pkl"):
    gdown.download("https://drive.google.com/uc?id=1fK7ItKl5GcJjxaw3M9IAP6gDyuQXstUz", "sentiment/stacking.pkl", quiet=False)

embedding_model = joblib.load("clustering/sentence_bert.pkl")

num_cols = ['Salary & benefits', 'Training & learning', 'Culture & fun',
            'Office & workspace', 'Management cares about me']

# ƒê·ªçc t·ª´ kh√≥a t√≠ch c·ª±c
pos_words = set(Path("files/positive_words.txt").read_text("utf-8").splitlines())

# ƒê·ªçc t·ª´ kh√≥a ti√™u c·ª±c
neg_words = set(Path("files/negative_words.txt").read_text("utf-8").splitlines())

print(f"> Loaded {len(pos_words)} positive, {len(neg_words)} negative keywords.")

# ƒê·ªçc emoji t√≠ch c·ª±c
pos_emoji = set(Path("files/positive_emoji.txt").read_text("utf-8").splitlines())

# ƒê·ªçc emoji ti√™u c·ª±c
neg_emoji = set(Path("files/negative_emoji.txt").read_text("utf-8").splitlines())

print(f"Loaded {len(pos_emoji)} positive emojis, {len(neg_emoji)} negative emojis")


# Stopwords ti·∫øng Vi·ªát c∆° b·∫£n
review_stopwords_vi = {
    'th√¨', 'c·ªßa', 'v√†', 'l√†', '·ªü', 'v·ªÅ', 'ta', 't√¥i', 'b·∫°n', 'anh', 'ch·ªã', 'em',
    'm√¨nh', 'c√°c', 'nh·ªØng', 'm·ªôt', 'hai', 'ba', 'n√†y', 'ƒë√≥', 'kia', 'cho', 't·ª´',
    'v·ªõi', 'trong', 'ngo√†i', 'tr√™n', 'd∆∞·ªõi', 'sau', 'tr∆∞·ªõc', 'gi·ªØa', 'b√™n', 'c·∫°nh',
    'r·∫•t', 'kh√°', 'h∆°i', 'qu√°', 'ƒë√£', 'ƒëang', 's·∫Ω', 'v·∫´n', 'c√≤n', 'ch∆∞a', 'kh√¥ng'
}

# Stopwords ti·∫øng Anh c∆° b·∫£n
review_stopwords_en = {
    'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
    'from', 'as', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',
    'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
    'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
    'my', 'your', 'his', 'her', 'its', 'our', 'their', 'me', 'him', 'us', 'them'
}

# T·ª´ nhi·ªÖu c·ª• th·ªÉ
noise_words = {
    'cai', 'chua', 'ro', 'rang', 'notthing', 'thien', 'xe', 'd·ªü', 'ch·ªó',
    'can', 'thing', 'process', 'need', 'improve' # ch·ªâ lo·∫°i n·∫øu ch√∫ng xu·∫•t hi·ªán ƒë·ªôc l·∫≠p
}

# C·∫•u h√¨nh hi·ªÉn th·ªã v√† t·∫Øt c·∫£nh b√°o
plt.rcParams['font.family'] = 'DejaVu Sans'
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
warnings.filterwarnings('ignore')

def clean_text(text):
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt
    text = re.sub(r'[^\w\s\.,!?;:]', ' ', text)

    # X√≥a URL
    text = re.sub(r'http\S+|www\.\S+', '', text)

    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text)

    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi
    text = text.strip()

    return text

def normalize_repeated_characters(text):
    """
    Chu·∫©n h√≥a c√°c t·ª´ c√≥ k√Ω t·ª± l·∫∑p li√™n ti·∫øp
    V√≠ d·ª•: "l√≤nggggg" -> "l√≤ng", "thi·ªátttt" -> "thi·ªát"
    """
    # X·ª≠ l√Ω k√Ω t·ª± Vi·ªát Nam (bao g·ªìm d·∫•u)
    # Thay th·∫ø 3+ k√Ω t·ª± li√™n ti·∫øp b·∫±ng 1 k√Ω t·ª±
    text = re.sub(r'([aƒÉ√¢e√™iou√¥∆°∆∞y√†√°·∫£√£·∫°·∫±·∫Ø·∫≥·∫µ·∫∑·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç·ªì·ªë·ªï·ªó·ªô·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ])\1{2,}', r'\1', text, flags=re.IGNORECASE)

    # X·ª≠ l√Ω consonant
    text = re.sub(r'([bcdfghjklmnpqrstvwxz])\1{2,}', r'\1', text, flags=re.IGNORECASE)

    return text

def normalize_punctuation(text):
    """Chu·∫©n h√≥a c√°c d·∫•u c√¢u"""

    # Chu·∫©n h√≥a d·∫•u ch·∫•m
    text = re.sub(r'\.{2,}', '.', text)

    # Chu·∫©n h√≥a d·∫•u h·ªèi ch·∫•m
    text = re.sub(r'\?{2,}', '?', text)

    # Chu·∫©n h√≥a d·∫•u c·∫£m th√°n
    text = re.sub(r'!{2,}', '!', text)

    # Chu·∫©n h√≥a d·∫•u ph·∫©y
    text = re.sub(r',{2,}', ',', text)

    # Lo·∫°i b·ªè d·∫•u nh√°y ƒë∆°n (n·∫øu c·∫ßn)
    text = text.replace("'", "")

    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng xung quanh d·∫•u c√¢u
    text = re.sub(r'\s*([.!?,:;])\s*', r'\1 ', text)
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

def normalize_vietnamese(text):
    """Chu·∫©n h√≥a unicode ti·∫øng Vi·ªát s·ª≠ d·ª•ng unicodedata"""
    return unicodedata.normalize('NFC', text)

def process_special_chars(text):
    # Decode HTML entities
    text = html.unescape(text)

    # X·ª≠ l√Ω emoji (t√πy task)
    # C√≥ th·ªÉ gi·ªØ nguy√™n ho·∫∑c thay th·∫ø b·∫±ng text m√¥ t·∫£

    return text

def process_basic_text(text, max_length=256):
    print("process_basic_text...")
    # 1. L√†m s·∫°ch c∆° b·∫£n
    text = clean_text(text)

    # 2. Chu·∫©n h√≥a k√Ω t·ª± l·∫∑p
    text = normalize_repeated_characters(text)

    # 3. Chu·∫©n h√≥a d·∫•u c√¢u
    text = normalize_punctuation(text)

    # 4. Chu·∫©n h√≥a ti·∫øng Vi·ªát
    text = normalize_vietnamese(text)

    # 5. X·ª≠ l√Ω k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = process_special_chars(text)

    return text

def calc_sentiment_features(text):
    toks = text.lower().strip()
    pos_w = sum(t in toks for t in pos_words)
    neg_w = sum(t in toks for t in neg_words)
    pos_e = sum(text.count(e) for e in pos_emoji)
    neg_e = sum(text.count(e) for e in neg_emoji)

    total_w = pos_w + neg_w
    total_e = pos_e + neg_e
    total_we = total_w + total_e

    ratio_words = (pos_w - neg_w) / total_w if total_w else 0
    ratio_emoji = (pos_e - neg_e) / total_e if total_e else 0
    ratio_all = (pos_w + pos_e - neg_w - neg_e) / total_we if total_we else 0

    return pos_w, neg_w, pos_e, neg_e, total_we, ratio_all

def detect_lang_safe(text):
    # H√†m detect language ƒë∆°n gi·∫£n, b·∫°n c√≥ th·ªÉ d√πng langdetect ho·∫∑c rule ri√™ng
    try:
        return detect(text)
    except:
        return ''

from collections import deque
def split_sentences_by_meaning(text, lang='vi'):
    """
    T√°ch c√¢u theo d·∫•u c√¢u v√† m·ªôt s·ªë li√™n t·ª´ chia √Ω.

    Args:
        text (str): Input text to split
        lang (str): Language code ('vi' for Vietnamese, 'en' for English)

    Returns:
        list: List of split sentences
    """
    # Danh s√°ch c√°c li√™n t·ª´ v√† t·ª´ n·ªëi ph·ªï bi·∫øn d√πng ƒë·ªÉ t√°ch √Ω
    if lang == 'vi':
        split_keywords = [
            r'\bnh∆∞ng\b', r'\btuy nhi√™n\b', r'\btuy\b', r'\bm·∫∑c d√π\b',
            r'\bd√π\b', r'\bv√¨ v·∫≠y\b', r'\bv√¨ th·∫ø\b', r'\bdo ƒë√≥\b',
            r'\bc≈©ng\b', r'\bsong\b',
        ]
    elif lang == 'en':
        split_keywords = [
            r'\bbut\b', r'\bhowever\b', r'\bnevertheless\b', r'\bnonetheless\b',
            r'\balthough\b', r'\bthough\b', r'\beven though\b', r'\bwhile\b',
            r'\bwhereas\b', r'\btherefore\b', r'\bthus\b', r'\bhence\b',
            r'\bconsequently\b', r'\bmoreover\b', r'\bfurthermore\b', r'\bin addition\b',
            r'\byet\b', r'\bso\b', r'\bfor\b', r'\bnor\b',
            r'\botherwise\b', r'\bmeanwhile\b', r'\bon the other hand\b',
            r'\bin contrast\b', r'\bsimilarly\b', r'\blikewise\b'
        ]
    else:
        # Default to Vietnamese keywords
        split_keywords = [
            r'\bnh∆∞ng\b', r'\btuy nhi√™n\b', r'\btuy\b', r'\bm·∫∑c d√π\b',
            r'\bd√π\b', r'\bv√¨ v·∫≠y\b', r'\bv√¨ th·∫ø\b', r'\bdo ƒë√≥\b',
            r'\bc≈©ng\b', r'\bsong\b',
        ]

    # B∆∞·ªõc 1: Chu·∫©n h√≥a vƒÉn b·∫£n
    text = text.strip()
    if not text:
        return []

    # B∆∞·ªõc 2: T√°ch c√¢u theo d·∫•u ng·∫Øt (., !, ?)
    sentences = re.split(r'(?<=[.!?])\s+', text)

    final_sentences = []

    # B∆∞·ªõc 3: V·ªõi m·ªói c√¢u, ti·∫øp t·ª•c t√°ch n·∫øu c√≥ li√™n t·ª´ mang nhi·ªÅu √Ω
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        # Gh√©p c√°c keyword l·∫°i th√†nh regex OR
        pattern = '|'.join(split_keywords)

        # T√°ch n·∫øu c√≥ t·ª´ n·ªëi nhi·ªÅu √Ω
        sub_sentences = re.split(pattern, sentence, flags=re.IGNORECASE)

        # Flatten the list of lists and strip each element
        for sub_sentence in sub_sentences:
            split_do_neu_result = recursive_split_sentences(sub_sentence, lang)
            final_sentences.extend([s.strip() for s in split_do_neu_result if s.strip()])

    return final_sentences

def recursive_split_sentences(text, lang='vi'):
    """
    Recursively split sentences based on patterns for the specified language.

    Args:
        text (str): Input text to split
        lang (str): Language code ('vi' for Vietnamese, 'en' for English)

    Returns:
        list: List of split sentences
    """
    # Define patterns for each language
    if lang == 'vi':
        patterns = [
            r'\bm·ªói t·ªôi\b.*?\bdo\b.*?(?=,|\.|$)',
            r'\bdo\b.*?\bn√™n\b.*?(?=,|\.|$)',
            r'\bv√¨\b.*?\bn√™n\b.*?(?=,|\.|$)',
            r'\bm·∫∑c d√π\b.*?\bnh∆∞ng\b.*?(?=,|\.|$)',
            r'\bn·∫øu\b.*?\bth√¨\b.*?(?=,|\.|$)',
        ]
    elif lang == 'en':
        patterns = [
            r'\bif\b.*?\bthen\b.*?(?=,|\.|$)',
            r'\bwhen\b.*?\bthen\b.*?(?=,|\.|$)',
            r'\bsince\b.*?\btherefore\b.*?(?=,|\.|$)',
            r'\bbecause\b.*?\bso\b.*?(?=,|\.|$)',
            r'\bas\b.*?\bso\b.*?(?=,|\.|$)',
            r'\balthough\b.*?\byet\b.*?(?=,|\.|$)',
            r'\bthough\b.*?\bstill\b.*?(?=,|\.|$)',
            r'\bwhile\b.*?\bhowever\b.*?(?=,|\.|$)',
            r'\bunless\b.*?\botherwise\b.*?(?=,|\.|$)',
            r'\beven if\b.*?\bstill\b.*?(?=,|\.|$)',
        ]
    else:
        # Default to Vietnamese patterns
        patterns = [
            r'\bm·ªói t·ªôi\b.*?\bdo\b.*?(?=,|\.|$)',
            r'\bdo\b.*?\bn√™n\b.*?(?=,|\.|$)',
            r'\bv√¨\b.*?\bn√™n\b.*?(?=,|\.|$)',
            r'\bm·∫∑c d√π\b.*?\bnh∆∞ng\b.*?(?=,|\.|$)',
            r'\bn·∫øu\b.*?\bth√¨\b.*?(?=,|\.|$)',
        ]

    queue = deque([text.strip()])
    results = []

    while queue:
        sentence = queue.popleft().strip(" ,.")

        matched = False
        for pattern in patterns:
            match = re.search(pattern, sentence, re.IGNORECASE)
            if match:
                matched = True
                # Ph·∫ßn tr∆∞·ªõc m·∫´u
                before = sentence[:match.start()].strip(" ,.")
                # Ph·∫ßn tr√πng v·ªõi m·∫´u
                middle = match.group().strip(" ,.")
                # Ph·∫ßn sau m·∫´u
                after = sentence[match.end():].strip(" ,.")

                if before:
                    queue.append(before)
                results.append(middle + ".")  # ƒê∆∞a th·∫≥ng v√†o k·∫øt qu·∫£, kh√¥ng x·ª≠ l√Ω l·∫°i
                if after:
                    queue.append(after)
                break  # Ch·ªâ d√πng 1 pattern m·ªói v√≤ng

        if not matched:
            if sentence:
                results.append(sentence + ".")

    return results

# H√†m ti·ªán √≠ch ƒë·ªÉ load t·∫•t c·∫£ d·ªØ li·ªáu
def load_processing_data():
    """Load t·∫•t c·∫£ d·ªØ li·ªáu c·∫ßn thi·∫øt cho vi·ªác x·ª≠ l√Ω vƒÉn b·∫£n"""

    # LOAD EMOJICON
    try:
        with open('files/emojicon.txt', 'r', encoding="utf8") as file:
            emoji_lst = file.read().split('\n')
        emoji_dict = {}
        for line in emoji_lst:
            if '\t' in line and line.strip():  # Ki·ªÉm tra format v√† kh√¥ng r·ªóng
                line = normalize_vietnamese(line)  # Chu·∫©n h√≥a unicode
                parts = line.split('\t', 1)  # Ch·ªâ split l·∫ßn ƒë·∫ßu
                if len(parts) == 2:
                    key, value = parts
                    emoji_dict[key] = str(value)
    except FileNotFoundError:
        print("Warning: emojicon.txt not found, using empty emoji dict")
        emoji_dict = {}

    # LOAD TEENCODE
    try:
        with open('files/teencode.txt', 'r', encoding="utf8") as file:
            teen_lst = file.read().split('\n')
        teen_dict = {}
        for line in teen_lst:
            if '\t' in line and line.strip():
                line = normalize_vietnamese(line)  # Chu·∫©n h√≥a unicode
                parts = line.split('\t', 1)
                if len(parts) == 2:
                    key, value = parts
                    teen_dict[key] = str(value)
    except FileNotFoundError:
        print("Warning: teencode.txt not found, using empty teen dict")
        teen_dict = {}

    #LOAD TRANSLATE ENGLISH -> VNMESE
    try:
        with open('files/english-vnmese.txt', 'r', encoding="utf8") as file:
            english_lst = file.read().split('\n')
        english_dict = {}
        for line in english_lst:
            if '\t' in line and line.strip():
                line = normalize_vietnamese(line)  # Chu·∫©n h√≥a unicode
                parts = line.split('\t', 1)
                if len(parts) == 2:
                    key, value = parts
                    english_dict[key] = str(value)
    except FileNotFoundError:
        print("Warning: teencode.txt not found, using empty teen dict")
        english_dict = {}

    # LOAD WRONG WORDS
    try:
        with open('files/wrong-word.txt', 'r', encoding="utf8") as file:
            wrong_lst = file.read().split('\n')
        wrong_lst = [normalize_vietnamese(word.strip()) for word in wrong_lst if word.strip()]  # Chu·∫©n h√≥a unicode
    except FileNotFoundError:
        print("Warning: wrong-word.txt not found, using empty wrong list")
        wrong_lst = []


    # LOAD STOPWORDS
    try:
        with open('files/vietnamese-stopwords.txt', 'r', encoding="utf8") as file:
            stopwords_vi = file.read().split('\n')
        stopwords_vi = [normalize_vietnamese(word.strip()) for word in stopwords_vi if word.strip()]  # Chu·∫©n h√≥a unicode
        # Lo·∫°i b·ªè c√°c t·ª´ ph·ªß ƒë·ªãnh trong stopword
        negations_vi = {'kh√¥ng', 'ch·∫≥ng', 'ch·∫£', 'ƒë√¢u', 'ch∆∞a', 'ƒë·ª´ng', 'kh·ªèi'}
        stopwords_vi = [word for word in stopwords_vi if word not in negations_vi]
        # Lo·∫£i b·ªè c√°c t·ª´ quan tr·ªçng
        keep_words = {'r·∫•t', 'kh√¥ng', 'n√™n', 'c√≥', 't·ªët', 'x·∫•u', 'l√†m vi·ªác', '√°p l·ª±c', 'tho·∫£i m√°i', '·ªïn', 'h√†i l√≤ng', 'kh√≥', 't·ªá', 'c·ª±c k·ª≥', 'nhi·ªÅu', '√≠t', 'cao', 'th·∫•p', 'kh√°'}
        stopwords_vi = [word for word in stopwords_vi if word not in keep_words]
    except FileNotFoundError:
        print("Warning: vietnamese-stopwords.txt not found, using empty stopwords list")
        stopwords_vi = []

    # L·∫•y b·ªô stopwords ti·∫øng Anh
    stopwords_en = set(stopwords.words('english'))

    # Lo·∫°i b·ªè t·ª´ ph·ªß ƒë·ªãnh kh·ªèi b·ªô stopwords
    negations = {'not', 'no', 'nor', 'don', 'didn', 'doesn', "don't", "didn't", "doesn't"}
    stopwords_en = [word for word in stopwords_en if word not in negations]
    # Lo·∫°i b·ªè c√°c t·ª´ quan tr·ªçng
    keep_words = {'not', 'very', 'no', 'never', 'good', 'bad', 'great', 'poor', 'excellent', 'happy', 'sad', 'stressful', 'comfortable', 'satisfied', 'unhappy', 'awful', 'amazing'}
    stopwords_en = [word for word in stopwords_en if word not in keep_words]

    return emoji_dict, teen_dict, english_dict, wrong_lst, stopwords_vi, stopwords_en

emoji_dict, teen_dict, english_dict, wrong_lst, stopwords_vi, stopwords_en = load_processing_data()
# K·∫øt h·ª£p cu·ªëi c√πng
all_stopwords = set(stopwords_vi) | set(stopwords_en) | review_stopwords_vi | review_stopwords_en | noise_words


def process_split_text(split_text, lang='vi'):
    """
    X·ª≠ l√Ω nhi·ªÅu c√¢u c√πng l√∫c

    """
    results = []
    for text in split_text:
        if not text or not text.strip():
            continue

        results.extend(process_text(text, lang))
    # lo·∫°i b·ªè c√¢u tr√πng
    results = list(set(results))
    # n·ªëi th√†nh 1 chu·ªói
    return '. '.join(results)

def process_text(text, lang='vi'):
    if not text or not text.strip():
        return ""

    # B∆∞·ªõc 1: Thay emoji th√†nh t·ª´ t∆∞∆°ng ·ª©ng
    text = replace_emoji(text, emoji_dict)

    # B∆∞·ªõc 2: Chu·∫©n h√≥a teencode
    text = normalize_text(text, teen_dict)

    if lang == 'vi':
        tokens = vi_tokenize(text, format="text")
        tagged = vi_pos_tag(tokens)
        results = process_tagged_sentence(tagged).split('.')
        results = [s.strip() for s in results if s.strip()]
    else:
        tokens = en_tokenize(text)
        tagged = en_pos_tag(tokens)
        results = process_tagged_sentence(tagged).split('.')
        results = [s.strip() for s in results if s.strip()]

    return results

def normalize_text(text, dict_map):
    # Thay th·∫ø t·ª´ theo dict_map (teencode, sai ch√≠nh t·∫£, emoji...)
    pattern = re.compile(r'\b(' + '|'.join(re.escape(key) for key in dict_map.keys()) + r')\b', flags=re.IGNORECASE)
    def replace_func(match):
        key = match.group(0).lower()
        return dict_map.get(key, key)
    return pattern.sub(replace_func, text)

def replace_emoji(text, emoji_dict):
    # Thay emoji t·ª´ng k√Ω t·ª±
    for emo, rep in emoji_dict.items():
        text = text.replace(emo, ' ' + rep + ' ')
    return text

def process_tagged_sentence(tagged_words, language='vi'):
    """
    X·ª≠ l√Ω c√¢u ƒë√£ ƒë∆∞·ª£c g√°n nh√£n t·ª´ lo·∫°i ƒë·ªÉ t·∫°o c√¢u c√≥ nghƒ©a v·ªõi ph√¢n c·ª•m √Ω nghƒ©a
    H·ªó tr·ª£ c·∫£ ti·∫øng Vi·ªát v√† ti·∫øng Anh

    Args:
        tagged_words: Danh s√°ch c√°c tuple (t·ª´, nh√£n_t·ª´_lo·∫°i)
        language: 'vi' cho ti·∫øng Vi·ªát, 'en' cho ti·∫øng Anh

    Returns:
        str: C√¢u ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω, ph√¢n th√†nh c√°c c·ª•m √Ω nghƒ©a
    """

    # ƒê·ªãnh nghƒ©a c√°c t·ª´ lo·∫°i c·∫ßn gi·ªØ l·∫°i (c√≥ nghƒ©a) cho c·∫£ ti·∫øng Vi·ªát v√† ti·∫øng Anh
    meaningful_tags = {
        'N', 'NN', 'NNS', 'NNP', 'NNPS',  # Danh t·ª´ (Noun)
        'V', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # ƒê·ªông t·ª´ (Verb)
        'A', 'JJ', 'JJR', 'JJS',  # T√≠nh t·ª´ (Adjective)
    }

    # ƒê·ªãnh nghƒ©a c√°c t·ª´ c·ª• th·ªÉ c·∫ßn lo·∫°i b·ªè theo ng√¥n ng·ªØ
    if language == 'vi':
        stop_words = {
            'c√≥_th·ªÉ', '·ªü', 't·ª´', 'v√¨', 'm·ªói', 'c·ªßa', 'v·ªõi', 'theo', 'trong', 'tr√™n', 'd∆∞·ªõi'
        }
        # Tr·∫°ng t·ª´ quan tr·ªçng c·∫ßn gi·ªØ
        important_adverbs = {'kh√¥ng', 'kh√°', 'r·∫•t', 't∆∞∆°ng_ƒë·ªëi', 'kh√°_l√†', 'ƒë√£', 's·∫Ω', 'ƒëang'}
        # T·ª´ quan tr·ªçng c·∫ßn gi·ªØ b·∫•t k·ªÉ t·ª´ lo·∫°i
        important_words = {'n√™n', 'v√†', 'ho·∫∑c', 'c≈©ng', 'th√¨', 'l√†', 'ƒë∆∞·ª£c'}
        # T·ª´ lo·∫°i d·∫•u c√¢u v√† gi·ªõi t·ª´ c·∫ßn b·ªè qua
        skip_tags = {'CH', 'E', 'C'}
    else:  # English
        stop_words = {
            'of', 'in', 'on', 'at', 'by', 'for', 'with', 'from', 'to', 'about',
            'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between'
        }
        # Tr·∫°ng t·ª´ quan tr·ªçng c·∫ßn gi·ªØ
        important_adverbs = {'not', 'very', 'quite', 'really', 'too', 'so', 'already', 'still', 'just'}
        # T·ª´ quan tr·ªçng c·∫ßn gi·ªØ b·∫•t k·ªÉ t·ª´ lo·∫°i
        important_words = {'and', 'or', 'but', 'should', 'must', 'can', 'will', 'would', 'could', 'is', 'are', 'was', 'were', 'be', 'been', 'being'}
        # T·ª´ lo·∫°i d·∫•u c√¢u v√† gi·ªõi t·ª´ c·∫ßn b·ªè qua
        skip_tags = {'.', ',', ':', ';', '!', '?', 'IN', 'TO', 'CC'}

    meaningful_words = []
    current_chunk = []
    check_tags = set()

    for i, (word, tag) in enumerate(tagged_words):
        # B·ªè qua c√°c t·ª´ trong danh s√°ch stop words
        if word.lower() in stop_words:
            continue

        # X·ª≠ l√Ω d·∫•u ph·∫©y - k·∫øt th√∫c c·ª•m hi·ªán t·∫°i
        if word == ',' and current_chunk:
            if len(check_tags) == 1 and meaningful_words:
                last_text = meaningful_words.pop()
                meaningful_words.append(last_text + ', ' + ' '.join(current_chunk))
            else:
                meaningful_words.append(' '.join(current_chunk))
            current_chunk = []
            check_tags = set()
            continue

        # B·ªè qua d·∫•u ch·∫•m cu·ªëi c√¢u
        if word == '.' and i == len(tagged_words) - 1:
            continue

        # Gi·ªØ l·∫°i t·ª´ quan tr·ªçng b·∫•t k·ªÉ t·ª´ lo·∫°i
        if word.lower() in important_words:
            current_chunk.append(word)
            check_tags.add(tag)
            continue

        # Gi·ªØ l·∫°i c√°c t·ª´ c√≥ nghƒ©a
        if tag in meaningful_tags:
            # Ki·ªÉm tra tr·∫°ng t·ª´ quan tr·ªçng
            if tag in {'R', 'RB', 'RBR', 'RBS'} and word.lower() not in important_adverbs:
                continue
            current_chunk.append(word)
            check_tags.add(tag)
        elif tag not in skip_tags:  # Gi·ªØ l·∫°i c√°c tag kh√°c n·∫øu kh√¥ng ph·∫£i d·∫•u c√¢u, gi·ªõi t·ª´, li√™n t·ª´
            current_chunk.append(word)
            check_tags.add(tag)

    # Th√™m c·ª•m cu·ªëi c√πng n·∫øu c√≤n
    if current_chunk:
        if len(check_tags) == 1 and meaningful_words:
              last_text = meaningful_words.pop()
              meaningful_words.append(last_text + ', ' + ' '.join(current_chunk))
        else:
            meaningful_words.append(' '.join(current_chunk))

    # H√†m vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu
    def capitalize_first_word(text):
        if not text:
            return text
        words = text.split()
        if words and len(words[0]) > 0:
            first_word = words[0]
            words[0] = first_word[0].upper() + first_word[1:]
        return ' '.join(words)

    # T·∫°o c√¢u v·ªõi ƒë·ªãnh d·∫°ng mong mu·ªën v√† vi·∫øt hoa
    if len(meaningful_words) == 1:
        return capitalize_first_word(meaningful_words[0]) + "."
    elif len(meaningful_words) == 2:
        return (capitalize_first_word(meaningful_words[0]) + ". " +
                capitalize_first_word(meaningful_words[1]) + ".")
    elif len(meaningful_words) >= 3:
        # N·ªëi 2 c·ª•m ƒë·∫ßu b·∫±ng d·∫•u ph·∫©y, c√°c c·ª•m sau b·∫±ng d·∫•u ch·∫•m
        result = capitalize_first_word(meaningful_words[0]) + ", " + capitalize_first_word(meaningful_words[1]) + ". "
        for chunk in meaningful_words[2:]:
            result += capitalize_first_word(chunk) + ". "
        return result.rstrip()

    return ""

def evaluate_multiple_models(X, k_range=range(2, 9), dbscan_eps=0.5, dbscan_min_samples=5):
    """So s√°nh KMeans, Agglomerative v√† DBSCAN v·ªõi c√°c ch·ªâ s·ªë ƒë√°nh gi√°"""
    results = []

    for k in k_range:
        # KMeans
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels_kmeans = kmeans.fit_predict(X)
        sil_kmeans = silhouette_score(X, labels_kmeans)
        db_kmeans = davies_bouldin_score(X, labels_kmeans)
        ch_kmeans = calinski_harabasz_score(X, labels_kmeans)
        results.append({
            'Model': 'KMeans', 'k': k,
            'Silhouette': sil_kmeans,
            'DaviesBouldin': db_kmeans,
            'CalinskiHarabasz': ch_kmeans
        })

        # Agglomerative
        agg = AgglomerativeClustering(n_clusters=k)
        labels_agg = agg.fit_predict(X)
        sil_agg = silhouette_score(X, labels_agg)
        db_agg = davies_bouldin_score(X, labels_agg)
        ch_agg = calinski_harabasz_score(X, labels_agg)
        results.append({
            'Model': 'Agglomerative', 'k': k,
            'Silhouette': sil_agg,
            'DaviesBouldin': db_agg,
            'CalinskiHarabasz': ch_agg
        })

    # DBSCAN - ch·ªâ ch·∫°y 1 l·∫ßn v√¨ kh√¥ng c√≥ k
    dbscan = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples)
    labels_dbscan = dbscan.fit_predict(X)
    if len(set(labels_dbscan)) > 1 and -1 not in set(labels_dbscan):
        sil_db = silhouette_score(X, labels_dbscan)
        db_db = davies_bouldin_score(X, labels_dbscan)
        ch_db = calinski_harabasz_score(X, labels_dbscan)
        results.append({
            'Model': 'DBSCAN', 'k': len(set(labels_dbscan)),
            'Silhouette': sil_db,
            'DaviesBouldin': db_db,
            'CalinskiHarabasz': ch_db
        })

    return pd.DataFrame(results)

# 7. G√°n clustering theo model t·ªët nh·∫•t
def get_cluster_labels(model_name, X, k):
    if model_name == 'KMeans':
        model = KMeans(n_clusters=k, random_state=42, n_init=10)
    elif model_name == 'Agglomerative':
        model = AgglomerativeClustering(n_clusters=k)
    else:
        raise ValueError("Model kh√¥ng h·ªó tr·ª£!")

    pred = model.fit_predict(X)

    return model, pred

def evaluate_clustering(X, labels, name):
    """ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng clustering"""
    silhouette = silhouette_score(X, labels)
    davies_bouldin = davies_bouldin_score(X, labels)
    calinski_harabasz = calinski_harabasz_score(X, labels)

    print(f"\n{name}:")
    print(f"  Silhouette Score: {silhouette:.3f}")
    print(f"  Davies-Bouldin Index: {davies_bouldin:.3f}")
    print(f"  Calinski-Harabasz Index: {calinski_harabasz:.3f}")

    return silhouette, davies_bouldin, calinski_harabasz

def analyze_cluster_content(df, cluster_col, text_col, n_samples=3):
    """Ph√¢n t√≠ch n·ªôi dung c·ªßa t·ª´ng cluster"""
    print(f"\n--- Ph√¢n t√≠ch {cluster_col} ---")

    for cluster_id in sorted(df[cluster_col].unique()):
        cluster_data = df[df[cluster_col] == cluster_id]
        print(f"\nCluster {cluster_id} (n={len(cluster_data)}):")

        # Hi·ªÉn th·ªã m·ªôt s·ªë m·∫´u
        samples = cluster_data[text_col].head(n_samples)
        for i, sample in enumerate(samples, 1):
            print(f"  {i}. {sample[:100]}...")

        # Th·ªëng k√™ rating
        print(f"  Trung b√¨nh rating:")
        for col in num_cols:
            if col in cluster_data.columns:
                avg_rating = cluster_data[col].mean()
                print(f"    {col}: {avg_rating:.2f}")

def plot_top_keywords_tfidf(df, cluster_col, text_col, top_n=20):
    for cluster_id in sorted(df[cluster_col].unique()):
        text = df[df[cluster_col] == cluster_id][text_col].dropna().tolist()
        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(6, 6), min_df=5, max_df=0.7, lowercase=True, analyzer='word', sublinear_tf=True)
        tfidf_matrix = vectorizer.fit_transform(text)
        tfidf_mean = tfidf_matrix.mean(axis=0).A1
        keywords = pd.Series(tfidf_mean, index=vectorizer.get_feature_names_out()).sort_values(ascending=False)[:top_n]

        print(f"\nCluster {cluster_id} top keywords:")
        print(keywords)

        # WordCloud
        wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(keywords)
        plt.figure(figsize=(10, 5))
        plt.imshow(wc, interpolation='bilinear')
        plt.axis('off')
        plt.title(f"WordCloud - Cluster {cluster_id}")
        plt.show()

def extract_textrank_keywords(text, top_n=15):
    """
    H√†m ri√™ng ƒë·ªÉ extract TextRank keywords v·ªõi nhi·ªÅu chi·∫øn l∆∞·ª£c
    """
    all_keywords = []

    try:
        # Chi·∫øn l∆∞·ª£c 1: L·∫•y c·ª•m t·ª´ (split=False)
        phrases = textrank_keywords.keywords(text, words=min(top_n, 10), split=False)
        if phrases:
            phrase_list = [p.strip() for p in phrases.split('\n') if p.strip()]
            all_keywords.extend(phrase_list)
    except:
        pass

    try:
        # Chi·∫øn l∆∞·ª£c 2: L·∫•y t·ª´ ƒë∆°n (split=True)
        words = textrank_keywords.keywords(text, words=min(top_n, 10), split=True)
        if words:
            all_keywords.extend(words)
    except:
        pass

    try:
        # Chi·∫øn l∆∞·ª£c 3: T·∫°o n-gram t·ª´ top words
        if len(all_keywords) > 3:
            # T·∫°o bigram v√† trigram t·ª´ c√°c t·ª´ quan tr·ªçng
            words_for_ngram = [w for w in all_keywords if len(w.split()) == 1][:8]
            text_words = text.lower().split()

            # T√¨m bigram
            for i in range(len(text_words) - 1):
                bigram = f"{text_words[i]} {text_words[i+1]}"
                if any(word in bigram for word in words_for_ngram):
                    all_keywords.append(bigram)

            # T√¨m trigram
            for i in range(len(text_words) - 2):
                trigram = f"{text_words[i]} {text_words[i+1]} {text_words[i+2]}"
                if any(word in trigram for word in words_for_ngram):
                    all_keywords.append(trigram)
    except:
        pass

    return all_keywords

def is_valid_keyword(kw):
    """C·∫£i thi·ªán validation cho keyword"""
    kw_clean = kw.strip()

    # Ki·ªÉm tra ƒë·ªô d√†i
    if len(kw_clean) <= 2: return False

    # Ki·ªÉm tra stopwords
    if kw_clean.lower() in all_stopwords: return False

    # Ki·ªÉm tra k√Ω t·ª± ƒë·∫∑c bi·ªát
    if re.match(r'^[\W_]+$', kw_clean): return False

    # Ki·ªÉm tra s·ªë
    if kw_clean.isdigit(): return False

    # Ki·ªÉm tra t·ª´ ƒë∆°n qu√° ng·∫Øn v√† ph·ªï bi·∫øn
    single_word_blacklist = {'work', 'good', 'great', 'nice', 'bad', 'ok', 'yes', 'no',
                            'c√¥ng', 'vi·ªác', 't·ªët', 'ƒë∆∞·ª£c', 'c√≥', 'l√†', 'v√†', 'kh√¥ng', 'cho'}
    if len(kw_clean.split()) == 1 and kw_clean.lower() in single_word_blacklist:
        return False

    # Ki·ªÉm tra c·ª•m t·ª´ k·∫øt th√∫c b·∫±ng d·∫•u c√¢u
    if kw_clean.endswith(('.', ':', ',', ';', '!', '?')):
        return False

    # Ki·ªÉm tra c·ª•m t·ª´ c√≥ √≠t nh·∫•t 1 t·ª´ c√≥ nghƒ©a (>= 3 k√Ω t·ª±)
    meaningful_words = [w for w in kw_clean.split() if len(w) >= 3]
    if len(meaningful_words) == 0:
        return False

    # Ki·ªÉm tra t·ª∑ l·ªá k√Ω t·ª± ƒë·∫∑c bi·ªát
    special_chars = sum(1 for c in kw_clean if not c.isalnum() and c != ' ')
    if special_chars > len(kw_clean) * 0.3:  # >30% k√Ω t·ª± ƒë·∫∑c bi·ªát
        return False

    return True

def clean_keyword(kw):
    """L√†m s·∫°ch keyword"""
    # Lo·∫°i b·ªè d·∫•u c√¢u ·ªü ƒë·∫ßu v√† cu·ªëi
    kw_clean = re.sub(r'^[\W_]+|[\W_]+$', '', kw.strip())

    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát th·ª´a
    kw_clean = re.sub(r'[^\w\s]', ' ', kw_clean)

    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    kw_clean = ' '.join(kw_clean.split())

    return kw_clean

def label_cluster_with_all_methods(df, cluster_col, text_col, top_n=10, display=True, batch_size=3):
    results = {}
    cluster_ids = sorted(df[cluster_col].unique())

    print(f"ƒêang x·ª≠ l√Ω {len(cluster_ids)} clusters c·ªßa {cluster_col}...")

    for i, cluster_id in enumerate(cluster_ids):
        print(f"X·ª≠ l√Ω cluster {cluster_id} ({i+1}/{len(cluster_ids)})")

        # L·ªçc data cho cluster hi·ªán t·∫°i
        cluster_data = df[df[cluster_col] == cluster_id].copy()
        cluster_data[text_col] = cluster_data.progress_apply(lambda x: split_sentences_by_meaning(x[text_col], x['lang']), axis=1)
        cluster_data[text_col] = cluster_data.progress_apply(lambda x: process_split_text(x[text_col], x['lang']), axis=1)


        # X·ª≠ l√Ω t·ª´ng ng√¥n ng·ªØ
        texts_vi = cluster_data[cluster_data['lang'] == 'vi'][text_col].dropna().tolist()
        texts_en = cluster_data[cluster_data['lang'] == 'en'][text_col].dropna().tolist()

        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng text ƒë·ªÉ ti·∫øt ki·ªám RAM
        texts_vi = texts_vi[:100]  # Ch·ªâ l·∫•y 50 text ƒë·∫ßu ti√™n
        texts_en = texts_en[:50]

        joined_text_vi = " ".join(texts_vi)
        joined_text_en = " ".join(texts_en)

        joined_text = joined_text_vi + " " + joined_text_en

        # B·ªè qua cluster n·∫øu text qu√° ng·∫Øn
        if len(joined_text.strip()) < 50:
            results[cluster_id] = ["insufficient_data"]
            continue

        try:
            # YAKE
            kw_yake_vi, kw_yake_en = [], []
            if joined_text_vi.strip():
                kw_yake_vi = yake.KeywordExtractor(lan="vi", n=3, top=min(top_n, 15)).extract_keywords(joined_text_vi)
            if joined_text_en.strip():
                kw_yake_en = yake.KeywordExtractor(lan="en", n=3, top=min(top_n, 15)).extract_keywords(joined_text_en)

            # Clean v√† filter YAKE keywords
            yake_keywords = []
            for kw, score in kw_yake_vi + kw_yake_en:
                cleaned = clean_keyword(kw)
                if cleaned and is_valid_keyword(cleaned):
                    yake_keywords.append(cleaned)
            set_yake = set(yake_keywords)

            # TextRank
            set_textrank = set()
            try:
                if len(joined_text.strip()) > 100:
                    all_textrank = extract_textrank_keywords(joined_text, top_n)
                    set_textrank = set([kw for kw in all_textrank if is_valid_keyword(kw)])
                    # Clean v√† filter TextRank keywords
                    textrank_keywords = []
                    for kw in all_textrank:
                        cleaned = clean_keyword(kw)
                        if cleaned and is_valid_keyword(cleaned):
                            textrank_keywords.append(cleaned)
                    set_textrank = set(textrank_keywords)
            except Exception as e:
                print(f"TextRank failed for cluster {cluster_id}: {e}")
                set_textrank = set()

            # T·ªïng h·ª£p v√† voting
            all_keywords = list(set_yake | set_textrank)
            voting_score = {}
            for kw in all_keywords:
                voting_score[kw] = (
                    (kw in set_yake) +
                    (kw in set_textrank)
                )

            # ∆Øu ti√™n c·ª•m ‚â•2 t·ª´ & voting >= 2
            final_labels = sorted(
                [kw for kw in all_keywords if len(kw.split()) >= 2 and voting_score[kw] >= 2],
                key=lambda x: (-voting_score[x], joined_text.find(x))
            )[:top_n]

            if not final_labels:  # fallback n·∫øu kh√¥ng c√≥ t·ª´ kh√≥a m·∫°nh
                final_labels = sorted(
                    [kw for kw in all_keywords if is_valid_keyword(kw)],
                    key=lambda x: (-voting_score.get(x, 0), -len(x), joined_text.find(x))
                )[:top_n]

            results[cluster_id] = final_labels

            if display:
                print(f"\nüìå Cluster {cluster_id} - Label g·ª£i √Ω: {final_labels}")
                print(f"‚Üí YAKE: {list(set_yake)[:3]}")
                print(f"‚Üí TextRank: {list(set_textrank)[:3]}")

        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω cluster {cluster_id}: {e}")
            results[cluster_id] = ["error_processing"]

    return results

def get_top_tfidf_keywords_by_cluster(df, cluster_col, text_col, top_n=10):
    results = {}
    # G·ªôp t·∫•t c·∫£ vƒÉn b·∫£n theo cluster
    grouped_text = df.groupby(cluster_col)[text_col + '_advance'].apply(lambda texts: ' '.join(texts.dropna()))

    vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(5, 8), min_df=2, max_df=1.0, lowercase=True, analyzer='word', sublinear_tf=True, stop_words=None)

    tfidf_matrix = vectorizer.fit_transform(grouped_text)
    feature_names = vectorizer.get_feature_names_out()

    for i, cluster in enumerate(grouped_text.index):
        row = tfidf_matrix[i].toarray().flatten()
        top_indices = row.argsort()[::-1][:top_n]
        keywords = [feature_names[j] for j in top_indices if row[j] > 0]
        results[cluster] = keywords

    return results

def get_best_label_by_embedding(reduced_data, cluster_labels, keyword_dict):
    cluster_names = {}
    for cluster_id in sorted(set(cluster_labels)):
        idx = np.where(cluster_labels == cluster_id)[0]
        centroid = reduced_data[idx].mean(axis=0).reshape(1, -1)

        keywords = keyword_dict.get(cluster_id, [])
        keyword_embeds = embedding_model.encode(keywords, convert_to_numpy=True)

        sims = cosine_similarity(keyword_embeds, centroid).flatten()
        best_keyword = keywords[np.argmax(sims)] if len(sims) > 0 else "N/A"

        cluster_names[cluster_id] = best_keyword

        print(f"üìå Cluster {cluster_id} ‚Üí Nh√£n ch·ªçn theo embedding: {best_keyword}")
    return cluster_names

def rank_keywords_by_similarity(reduced_data, cluster_labels, keyword_dict):
    cluster_rankings = {}

    for cluster_id in sorted(set(cluster_labels)):
        idx = np.where(cluster_labels == cluster_id)[0]
        centroid = reduced_data[idx].mean(axis=0).reshape(1, -1)

        keywords = keyword_dict.get(cluster_id, [])
        if not keywords:
            cluster_rankings[cluster_id] = []
            continue

        keyword_embeds = embedding_model.encode(keywords, convert_to_numpy=True)
        sims = cosine_similarity(keyword_embeds, centroid).flatten()

        ranked = sorted(zip(keywords, sims), key=lambda x: -x[1])
        cluster_rankings[cluster_id] = ranked

        print(f"\nüìå Cluster {cluster_id} - Ranking:")
        for kw, sim in ranked:
            print(f"{kw:30s} ‚Üí similarity: {sim:.4f}")

    return cluster_rankings

def get_representative_samples(reduced_data, cluster_labels, df_text, top_n=5):
    cluster_samples = {}

    for cluster_id in sorted(set(cluster_labels)):
        idx = np.where(cluster_labels == cluster_id)[0]
        centroid = reduced_data[idx].mean(axis=0).reshape(1, -1)
        cluster_vecs = reduced_data[idx]
        sims = cosine_similarity(cluster_vecs, centroid).flatten()

        top_indices = idx[np.argsort(-sims)[:top_n]]
        samples = df_text.iloc[top_indices].tolist()

        print(f"\nüìå Top {top_n} reviews g·∫ßn trung t√¢m Cluster {cluster_id}:")
        for i, s in enumerate(samples, 1):
            print(f"{i}. {s[:150]}...")  # In 150 k√Ω t·ª± ƒë·∫ßu
        cluster_samples[cluster_id] = samples

    return cluster_samples

def summarize_clusters(cluster_labels, liked_reduced, keyword_rankings, df_text):
    summary = []
    cluster_ids = sorted(set(cluster_labels))

    for cluster_id in cluster_ids:
        keywords = [kw for kw, _ in keyword_rankings.get(cluster_id, [])[:5]]
        size = sum(cluster_labels == cluster_id)

        # Tr√≠ch 1-2 c√¢u ƒë·∫°i di·ªán
        idx = np.where(cluster_labels == cluster_id)[0]
        centroid = liked_reduced[idx].mean(axis=0).reshape(1, -1)
        cluster_vecs = liked_reduced[idx]
        sims = cosine_similarity(cluster_vecs, centroid).flatten()
        top_indices = idx[np.argsort(-sims)[:2]]
        sample_preview = [df_text.iloc[i][:100] + "..." for i in top_indices]

        summary.append({
            "Cluster": cluster_id,
            "Top Keywords": ", ".join(keywords),
            "Samples": "\n".join(sample_preview),
            "Size": size
        })

    return pd.DataFrame(summary)

def show_examples(df, company, liked_c=None, suggested_c=None, n=3):
    filtered = df[df['Company Name'] == company]
    if liked_c is not None:
        filtered = filtered[filtered['liked_cluster'] == liked_c]
    if suggested_c is not None:
        filtered = filtered[filtered['suggested_cluster'] == suggested_c]

    display_cols = ['Company Name', 'What I liked_procced', 'Suggestions for improvement_procced', 'Rating']
    return filtered[display_cols].head(n)

def find_contradictions(liked_counts, suggested_counts, company):
    liked = liked_counts.loc[company] if company in liked_counts.index else None
    suggested = suggested_counts.loc[company] if company in suggested_counts.index else None

    print(f"\nüìå Company: {company}")
    print(f"Liked Clusters: \n{liked}")
    print(f"Suggested Clusters: \n{suggested}")

def get_key_words(data, top_n=20, min_score=0.05, ngram_ranges=[(2,4), (1, 2)]):
    text = " ".join(data)  # G·ªôp danh s√°ch th√†nh chu·ªói

    if text.strip() == '':
        return pd.Series(dtype=float)

    if isinstance(text, str):
        text = [text]
    texts = [t.strip() for t in text if t.strip()]

    for ngram in ngram_ranges:
        print(f"\nTh·ª≠ ngram_range={ngram}:")
        vectorizer = TfidfVectorizer(ngram_range=ngram, stop_words=None)
        try:
            tfidf_matrix = vectorizer.fit_transform(texts)
            if tfidf_matrix.shape[1] == 0:
                print("‚ö†Ô∏è Kh√¥ng c√≥ t·ª´ n√†o ph√π h·ª£p.")
                continue

            feature_names = vectorizer.get_feature_names_out()
            tfidf_mean = tfidf_matrix.mean(axis=0).A1

            top_features = pd.Series(tfidf_mean, index=feature_names).sort_values(ascending=False)

            # ‚úÖ L·ªçc t·ª´ kh√≥a theo ng∆∞·ª°ng ƒëi·ªÉm
            filtered = top_features[top_features >= min_score][:top_n]
            top_features.index = top_features.index.astype(str)

            print(f"S·ªë t·ª´ v·ª±ng: {len(feature_names)}")
            print(f"Top {top_n} t·ª´ kh√≥a (l·ªçc theo min_score={min_score}):\n{filtered}")

            return filtered
        except Exception as e:
            print(f"L·ªói v·ªõi ngram_range={ngram}: {e}")

    return pd.Series(dtype=float)

def check_wordcloud(keywords, col_name):
    """T·∫°o WordCloud t·ª´ d·ªØ li·ªáu text"""
    if keywords.empty:
        print("‚ö†Ô∏è Kh√¥ng c√≥ t·ª´ kh√≥a ƒë·ªÉ t·∫°o WordCloud.")
        return None

    wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(keywords)
    # T·∫°o figure v√† v·∫Ω WordCloud
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wc, interpolation='bilinear')
    ax.axis("off")
    ax.set_title("WordCloud c·ªßa " + col_name, fontsize=16, fontweight='bold', pad=20)

    return fig

